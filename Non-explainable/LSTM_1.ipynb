{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab6faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/utils\")\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/config\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8cd820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_1</th>\n",
       "      <th>skill_2</th>\n",
       "      <th>skill_3</th>\n",
       "      <th>skill_5</th>\n",
       "      <th>skill_6</th>\n",
       "      <th>skill_7</th>\n",
       "      <th>skill_8</th>\n",
       "      <th>skill_9</th>\n",
       "      <th>skill_12</th>\n",
       "      <th>skill_13</th>\n",
       "      <th>...</th>\n",
       "      <th>skill_3926</th>\n",
       "      <th>skill_3927</th>\n",
       "      <th>skill_3928</th>\n",
       "      <th>skill_3929</th>\n",
       "      <th>skill_3930</th>\n",
       "      <th>skill_3931</th>\n",
       "      <th>skill_3932</th>\n",
       "      <th>skill_3933</th>\n",
       "      <th>skill_3934</th>\n",
       "      <th>skill_3935</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84432</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              skill_1  skill_2  skill_3  skill_5  skill_6  skill_7  skill_8  \\\n",
       "candidate_id                                                                  \n",
       "84267               0        0        0        0        0        0        0   \n",
       "84349               1        0        0        0        0        0        0   \n",
       "84381               0        0        0        0        0        0        0   \n",
       "84386               0        0        0        0        0        0        0   \n",
       "84432               0        0        0        0        0        0        0   \n",
       "\n",
       "              skill_9  skill_12  skill_13  ...  skill_3926  skill_3927  \\\n",
       "candidate_id                               ...                           \n",
       "84267               0         0         0  ...           0           0   \n",
       "84349               0         0         0  ...           0           0   \n",
       "84381               0         0         0  ...           0           0   \n",
       "84386               0         0         0  ...           0           0   \n",
       "84432               0         0         0  ...           0           0   \n",
       "\n",
       "              skill_3928  skill_3929  skill_3930  skill_3931  skill_3932  \\\n",
       "candidate_id                                                               \n",
       "84267                  0           0           0           0           0   \n",
       "84349                  0           0           0           0           0   \n",
       "84381                  0           0           0           0           0   \n",
       "84386                  0           0           0           0           0   \n",
       "84432                  0           0           0           0           0   \n",
       "\n",
       "              skill_3933  skill_3934  skill_3935  \n",
       "candidate_id                                      \n",
       "84267                  0           0           0  \n",
       "84349                  0           0           0  \n",
       "84381                  0           0           0  \n",
       "84386                  0           0           0  \n",
       "84432                  0           0           0  \n",
       "\n",
       "[5 rows x 317 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_csv(\"../Data/skills_one-hot.csv\").set_index(\"candidate_id\")\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a862170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = dict(zip(skills.index, skills.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1c6b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>W4</th>\n",
       "      <th>W5</th>\n",
       "      <th>W7</th>\n",
       "      <th>W9</th>\n",
       "      <th>WB</th>\n",
       "      <th>WC</th>\n",
       "      <th>WD</th>\n",
       "      <th>WE</th>\n",
       "      <th>WF</th>\n",
       "      <th>ZW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84603</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1  10  11  12  13  14  15  16  17  18  ...  W4  W5  W7  W9  WB  \\\n",
       "candidate_id                                         ...                       \n",
       "84603         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "84867         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "85035         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "85102         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "85214         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "\n",
       "              WC  WD  WE  WF  ZW  \n",
       "candidate_id                      \n",
       "84603          0   0   0   0   0  \n",
       "84867          0   0   0   0   0  \n",
       "85035          0   0   0   0   0  \n",
       "85102          0   0   0   0   0  \n",
       "85214          0   0   0   0   0  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certs = pd.read_csv(\"../Data/candidate_certificates_one-hot.csv\").set_index(\"candidate_id\")\n",
    "certs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14dcdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "certs = dict(zip(certs.index, certs.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab430f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = json.load(open(\"../Data/embeddings.json\"))\n",
    "# Convert to ints\n",
    "w2v = {int(k):{int(k2):v2 for k2, v2 in v.items()} for k, v in w2v.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b89ff263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(\"../Data/df_pred.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bbf799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>job_order</th>\n",
       "      <th>time_between</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>isco_functie_niveau</th>\n",
       "      <th>education</th>\n",
       "      <th>function_id</th>\n",
       "      <th>isco_code4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84556</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>156</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84556</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84556</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>275</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84556</td>\n",
       "      <td>3</td>\n",
       "      <td>1155</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84556</td>\n",
       "      <td>4</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   candidate_id  job_order  time_between  time_spent  isco_functie_niveau  \\\n",
       "0         84556          0            37         156                  2.0   \n",
       "1         84556          1            23         116                  1.0   \n",
       "2         84556          2            23         275                  2.0   \n",
       "3         84556          3          1155           4                  1.0   \n",
       "4         84556          4           203          11                  1.0   \n",
       "\n",
       "   education  function_id  isco_code4  \n",
       "0        0.0        937.0       207.0  \n",
       "1        0.0        811.0       347.0  \n",
       "2        0.0        937.0       207.0  \n",
       "3        0.0       1521.0       343.0  \n",
       "4        0.0       1521.0       343.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd9dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "career_paths = df_pred.groupby(\"candidate_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f7a36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(df_pred[\"isco_code4\"].unique())\n",
    "num_features = len(career_paths.mean().columns)\n",
    "num_classes, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20c39c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 465468/465468 [00:45<00:00, 10280.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to 2d-arrays, getting rid of candidate_ids as values\n",
    "career_paths = career_paths.progress_apply(lambda x: x.values[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e001de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop careers that are longer than 100 jobs or only 1 job\n",
    "career_lens = career_paths.apply(len)\n",
    "career_paths = career_paths.loc[(career_lens <= 101) & (career_lens > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398b8cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidate_id\n",
       "84556    [[0.0, 37.0, 156.0, 2.0, 0.0, 937.0, 207.0], [...\n",
       "84612    [[0.0, 2537.0, 6.0, 1.0, 0.0, 1521.0, 343.0], ...\n",
       "84731    [[0.0, 46.0, 23.0, 1.0, 0.0, 1521.0, 343.0], [...\n",
       "85437    [[0.0, 747.0, 670.0, 1.0, 2.0, 1521.0, 343.0],...\n",
       "85627    [[0.0, 140.0, 4627.0, 3.0, 0.0, 1922.0, 155.0]...\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "career_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "625ef384",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "candidate_lens = defaultdict(int)\n",
    "\n",
    "# max_skills = len([col for col in df_pred if \"skill_\" in col])\n",
    "\n",
    "for idx, career in zip(career_paths.index, career_paths.values):\n",
    "    label = career[-1, 6]\n",
    "    \n",
    "    if not np.isnan(label):\n",
    "        candidate_lens[idx] = len(career) - 1\n",
    "        \n",
    "        idxs.append(idx)\n",
    "        x.append(career[:-1].reshape(len(career) - 1, num_features))\n",
    "        y.append(label)\n",
    "\n",
    "idxs = np.array(idxs)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ed9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill = np.zeros([len(x), len(max(x, key = lambda x: len(x))), num_features])\n",
    "\n",
    "for i,j in enumerate(x):\n",
    "    if len(j):\n",
    "        to_fill[i][-len(j):] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113304bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(max(x, key = lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c62cf1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pred\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5761fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bda7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265309, 265309)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_fill), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a48263c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_fill = to_fill[:75000]\n",
    "# y = y[:75000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1fa4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "split = 0.8\n",
    "random.seed(42)\n",
    "\n",
    "training = np.array(random.sample(range(len(to_fill)), int(split * len(to_fill))))\n",
    "test = np.array(list(set(range(len(to_fill))) - set(training)))\n",
    "\n",
    "train_indices, val_indices = idxs[training], idxs[test]\n",
    "X_train, X_val = to_fill[training], to_fill[test]\n",
    "y_train, y_val = y[training].astype(int), y[test].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eebf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "counts = np.bincount(y_train) + 1\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[y_train]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# Create dataloaders\n",
    "train_data = TensorDataset(torch.Tensor(train_indices), \n",
    "                           torch.Tensor(X_train), \n",
    "                           torch.Tensor(y_train).type(torch.LongTensor))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=512, sampler=sampler)\n",
    "\n",
    "val_data = TensorDataset(torch.Tensor(val_indices),\n",
    "                         torch.Tensor(X_val),\n",
    "                         torch.Tensor(y_val).type(torch.LongTensor))\n",
    "\n",
    "valloader = DataLoader(val_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d497092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, embedding_size, \n",
    "                 skills, certs, w2v, candidate_lengths, max_len):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "              \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size + 317 + 98 + 300\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.LSTMs = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            input_size = self.input_size if i == 0 else hidden_size\n",
    "            self.LSTMs.append(nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                      num_layers=1, batch_first=True))\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.embedding = nn.Embedding(2, embedding_size)\n",
    "        \n",
    "        self.skill_keys = set(skills.keys())\n",
    "        self.skills = np.vectorize(skills.get)\n",
    "        \n",
    "        self.certs_keys = set(certs.keys())\n",
    "        self.certs = np.vectorize(certs.get)\n",
    "        \n",
    "        self.w2v_keys = set(w2v.keys())\n",
    "        self.w2v = w2v\n",
    "        \n",
    "        self.candidate_lengths = candidate_lens\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # self.conv = nn.Conv1d(613, 3, 1)\n",
    "\n",
    "    def forward(self, candidate, x):       \n",
    "        outputs = []\n",
    "        \n",
    "        # Default width of a row (filled with 0s)\n",
    "        width = torch.Tensor([0] * (317 + 98 + 300)).type(torch.LongTensor).to(device)\n",
    "        \n",
    "        extra_features = []\n",
    "                \n",
    "        # For each candidate in the current batch\n",
    "        for c in candidate:\n",
    "            career_duration = self.candidate_lengths[c.item()]            \n",
    "            \n",
    "            # Look up skills            \n",
    "            if c.item() in self.skill_keys:\n",
    "                skill_list = torch.Tensor(self.skills(c.item())).type(torch.LongTensor).to(device)\n",
    "            else:\n",
    "                skill_list = torch.Tensor([0] * 317).type(torch.LongTensor).to(device)\n",
    "\n",
    "            # Look up certificates\n",
    "            if c.item() in self.certs_keys:\n",
    "                certs_list = torch.Tensor(self.certs(c.item())).type(torch.LongTensor).to(device)\n",
    "            else:\n",
    "                certs_list = torch.Tensor([0] * 98).type(torch.LongTensor).to(device)\n",
    "                \n",
    "            ez_w2v = True\n",
    "            w2v_list = torch.Tensor([0] * 300).type(torch.LongTensor).to(device)\n",
    "            \n",
    "            # Look for cvs\n",
    "            if c.item() in self.w2v_keys:\n",
    "                cvs = self.w2v[c.item()]\n",
    "                \n",
    "                storage = []\n",
    "                \n",
    "                 # If a candidate only has one CV, proceed as normal\n",
    "                if len(cvs.keys()) == 1:\n",
    "                    w2v_list = torch.Tensor(cvs[0]).type(torch.LongTensor).to(device)\n",
    "                else: # Otherwise, stack them accordingly\n",
    "                    ez_w2v = False\n",
    "                    ks = np.array(list(cvs.keys()))\n",
    "                    \n",
    "                    # Find how many time steps (rows) each CV lasted\n",
    "                    durations = [ks[i+1] - ks[i]\n",
    "                                 if i < (len(ks) - 1) \n",
    "                                 else career_duration - ks[i]\n",
    "                                 for i in range(len(ks))]\n",
    "                                        \n",
    "                    embed_values = list(cvs.values())\n",
    "                    \n",
    "                    # When the CV got updated on the last timestep, aka our test value\n",
    "                    # Remove it from the list of durations, as it should be ignored\n",
    "                    if durations[-1] == 0: \n",
    "                        durations.pop()\n",
    "                    if durations[-1] == -1: # Sometimes contains -1 --> last location > (career duration)?\n",
    "                        durations.pop()\n",
    "                        durations[-1] -= 1\n",
    "                        # In case the last one should be ignored completely\n",
    "                        if durations[-1] == 0:\n",
    "                            durations.pop()\n",
    "                                               \n",
    "                    if durations:\n",
    "                        for i, duration in enumerate(durations):\n",
    "                            storage.append(torch.stack([torch.Tensor(embed_values[i])] * duration, dim=0))\n",
    "                    else:\n",
    "                        w2v_list = torch.Tensor(cvs[0]).type(torch.LongTensor).to(device)\n",
    "\n",
    "                    # Combine stored tensors into a single tensor\n",
    "                    cv_embeddings = torch.cat((storage)).type(torch.LongTensor).to(device)\n",
    "            else:\n",
    "                w2v_list = torch.Tensor([0] * 300).type(torch.LongTensor).to(device)\n",
    "\n",
    "            # Only create zeros if needed (e.g. less than max_len career duration)\n",
    "            if (self.max_len - career_duration) > 0:\n",
    "                zeros = torch.stack([width] * (self.max_len - career_duration))\n",
    "            else: # Reset zeros to prevent shape mismatch\n",
    "                zeros = torch.Tensor([]).type(torch.LongTensor).to(device)\n",
    "                \n",
    "            # If our word2vec is only one row like skills and certs\n",
    "            if ez_w2v:\n",
    "                # Fill a max_len row tensor with the correct values\n",
    "                combined = torch.cat([skill_list, certs_list, w2v_list], dim=-1)\n",
    "                \n",
    "                # Combine the features\n",
    "                filled_in = torch.stack([combined] * (career_duration))\n",
    "            else: # Otherwise, we first broadcast it, and then concatenate it afterwards\n",
    "                combined = torch.cat([skill_list, certs_list], dim=-1)\n",
    "                sk_ct = torch.stack([combined] * career_duration)                                             \n",
    "                filled_in = torch.cat([sk_ct, cv_embeddings], axis=1)\n",
    "                \n",
    "            total = torch.cat([zeros, filled_in], dim=0)\n",
    "                    \n",
    "            # Store result\n",
    "            extra_features.append(total)\n",
    "                                \n",
    "        # Convert list of tensors to actual tensor\n",
    "        additional_features = torch.stack((extra_features)).type(torch.FloatTensor).to(device)\n",
    "                \n",
    "        # Add features\n",
    "        x = torch.cat([x, additional_features], dim=2)\n",
    "        \n",
    "        # Forward pass\n",
    "        for i in range(self.num_layers):\n",
    "            output, (h_n, c_n) = self.LSTMs[i](x)\n",
    "            outputs.append(output)\n",
    "            x = output\n",
    "\n",
    "        h_out = h_n.view(-1, self.hidden_size)\n",
    "        out = self.fc(h_out)\n",
    "                        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "848e1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, trainloader, valloader, optimizer, criterion, num_epochs):\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (candidate, career, job) in enumerate(trainloader):\n",
    "            \n",
    "            candidate, career, job = candidate.to(device), career.to(device), job.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(candidate, career)\n",
    "\n",
    "            # obtain the loss function\n",
    "            loss = criterion(outputs, job)\n",
    "            loss = loss.mean()           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(\"Epoch: %d, batch: %d/%d, loss: %1.5f\" % (epoch + 1, i + 1, len(trainloader), loss.item()), end=\"\\r\")\n",
    "        \n",
    "        stats = test_loop(valloader, model, criterion)\n",
    "        results[\"Epoch\"].append(epoch + 1)\n",
    "        results[\"Acc@1\"].append(stats[0])\n",
    "        results[\"Acc@5\"].append(stats[1])\n",
    "        results[\"Acc@10\"].append(stats[2])\n",
    "        results[\"Acc@20\"].append(stats[3])\n",
    "        results[\"test_loss\"].append(stats[4])\n",
    "                \n",
    "    return results\n",
    "        \n",
    "def test_loop(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, acc1, acc5, acc10, acc20 = 0, 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for candidate, career, job in dataloader:\n",
    "            candidate, career, job = candidate.to(device), career.to(device), job.to(device)\n",
    "            pred = model(candidate, career)\n",
    "            \n",
    "            test_loss += criterion(pred, job).mean().item()\n",
    "            acc1 += (pred.argmax(1) == job).type(torch.float).sum().item()\n",
    "            \n",
    "            sorted_preds = torch.argsort(pred, 1, descending=True)\n",
    "            \n",
    "            acc5 += np.array([job[i].item() in sorted_preds[:,:5][i] for i in range(len(job))]).sum()\n",
    "            acc10 += np.array([job[i].item() in sorted_preds[:,:10][i] for i in range(len(job))]).sum()\n",
    "            acc20 += np.array([job[i].item() in sorted_preds[:,:20][i] for i in range(len(job))]).sum()\n",
    "\n",
    "            \n",
    "    # print(\"\\nValidation:\", Counter(np.array(pred.argmax(1).cpu())))\n",
    "    test_loss /= num_batches\n",
    "    acc1 /= size\n",
    "    acc5 /= size\n",
    "    acc10 /= size\n",
    "    acc20 /= size\n",
    "    print(f\"\\nTest Error:\")\n",
    "    print(f\"Acc@1: {(100*acc1):>0.2f}%, Acc@5: {100*acc5:>0.2f}%, \" +\\\n",
    "          f\"Acc@10: {100*acc10:>0.2f}%, Acc@20: {100*acc20:>0.2f}% Avg loss: {test_loss:>8f}\\n\")\n",
    "    \n",
    "    return acc1, acc5, acc10, acc20, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 0/64\n",
      "- Learning rate: 0.001\n",
      "- Model: \n",
      "\n",
      " LSTM(\n",
      "  (LSTMs): ModuleList(\n",
      "    (0): LSTM(722, 200, batch_first=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=200, out_features=354, bias=True)\n",
      "  (embedding): Embedding(2, 5)\n",
      ") \n",
      "\n",
      "Epoch: 1, batch: 415/415, loss: 4.20853\n",
      "Test Error:\n",
      "Acc@1: 7.14%, Acc@5: 21.44%, Acc@10: 30.33%, Acc@20: 41.53% Avg loss: 4.821265\n",
      "\n",
      "Epoch: 2, batch: 415/415, loss: 3.80269\n",
      "Test Error:\n",
      "Acc@1: 11.31%, Acc@5: 27.13%, Acc@10: 35.81%, Acc@20: 47.28% Avg loss: 4.564900\n",
      "\n",
      "Epoch: 3, batch: 415/415, loss: 3.36696\n",
      "Test Error:\n",
      "Acc@1: 11.23%, Acc@5: 28.07%, Acc@10: 37.35%, Acc@20: 49.45% Avg loss: 4.468983\n",
      "\n",
      "Epoch: 4, batch: 415/415, loss: 3.17754\n",
      "Test Error:\n",
      "Acc@1: 13.19%, Acc@5: 31.88%, Acc@10: 41.90%, Acc@20: 54.31% Avg loss: 4.318421\n",
      "\n",
      "Epoch: 5, batch: 415/415, loss: 3.02934\n",
      "Test Error:\n",
      "Acc@1: 14.13%, Acc@5: 33.32%, Acc@10: 43.61%, Acc@20: 55.71% Avg loss: 4.267223\n",
      "\n",
      "Epoch: 6, batch: 415/415, loss: 2.86102\n",
      "Test Error:\n",
      "Acc@1: 12.81%, Acc@5: 32.27%, Acc@10: 43.04%, Acc@20: 54.79% Avg loss: 4.277167\n",
      "\n",
      "Epoch: 7, batch: 415/415, loss: 2.99829\n",
      "Test Error:\n",
      "Acc@1: 16.95%, Acc@5: 37.18%, Acc@10: 47.91%, Acc@20: 59.95% Avg loss: 4.096726\n",
      "\n",
      "Epoch: 8, batch: 415/415, loss: 2.57762\n",
      "Test Error:\n",
      "Acc@1: 14.06%, Acc@5: 34.86%, Acc@10: 45.33%, Acc@20: 57.56% Avg loss: 4.176552\n",
      "\n",
      "Epoch: 9, batch: 415/415, loss: 2.61909\n",
      "Test Error:\n",
      "Acc@1: 13.67%, Acc@5: 34.46%, Acc@10: 45.38%, Acc@20: 57.96% Avg loss: 4.188268\n",
      "\n",
      "Epoch: 10, batch: 415/415, loss: 2.37591\n",
      "Test Error:\n",
      "Acc@1: 13.24%, Acc@5: 34.38%, Acc@10: 45.55%, Acc@20: 58.96% Avg loss: 4.163426\n",
      "\n",
      "Epoch: 11, batch: 415/415, loss: 2.43770\n",
      "Test Error:\n",
      "Acc@1: 14.16%, Acc@5: 35.78%, Acc@10: 46.63%, Acc@20: 59.70% Avg loss: 4.141933\n",
      "\n",
      "Epoch: 12, batch: 415/415, loss: 2.59885\n",
      "Test Error:\n",
      "Acc@1: 15.16%, Acc@5: 37.14%, Acc@10: 48.10%, Acc@20: 61.18% Avg loss: 4.079927\n",
      "\n",
      "Epoch: 13, batch: 415/415, loss: 2.49727\n",
      "Test Error:\n",
      "Acc@1: 15.73%, Acc@5: 38.50%, Acc@10: 49.21%, Acc@20: 61.56% Avg loss: 4.056355\n",
      "\n",
      "Epoch: 14, batch: 415/415, loss: 2.23391\n",
      "Test Error:\n",
      "Acc@1: 16.48%, Acc@5: 38.65%, Acc@10: 49.62%, Acc@20: 62.29% Avg loss: 4.019352\n",
      "\n",
      "Epoch: 15, batch: 415/415, loss: 2.26247\n",
      "Test Error:\n",
      "Acc@1: 16.05%, Acc@5: 39.65%, Acc@10: 51.00%, Acc@20: 63.60% Avg loss: 3.988587\n",
      "\n",
      "Epoch: 16, batch: 415/415, loss: 2.14030\n",
      "Test Error:\n",
      "Acc@1: 17.59%, Acc@5: 40.69%, Acc@10: 51.55%, Acc@20: 63.63% Avg loss: 3.958198\n",
      "\n",
      "Epoch: 17, batch: 415/415, loss: 2.23851\n",
      "Test Error:\n",
      "Acc@1: 15.83%, Acc@5: 39.61%, Acc@10: 50.79%, Acc@20: 64.03% Avg loss: 3.991633\n",
      "\n",
      "Epoch: 18, batch: 415/415, loss: 2.22605\n",
      "Test Error:\n",
      "Acc@1: 17.88%, Acc@5: 41.67%, Acc@10: 52.07%, Acc@20: 64.15% Avg loss: 3.935220\n",
      "\n",
      "Epoch: 19, batch: 415/415, loss: 2.19339\n",
      "Test Error:\n",
      "Acc@1: 14.90%, Acc@5: 38.25%, Acc@10: 49.51%, Acc@20: 62.39% Avg loss: 4.058233\n",
      "\n",
      "Epoch: 20, batch: 415/415, loss: 1.90418\n",
      "Test Error:\n",
      "Acc@1: 16.82%, Acc@5: 40.29%, Acc@10: 51.59%, Acc@20: 64.40% Avg loss: 3.943791\n",
      "\n",
      "Current iteration 1/64\n",
      "- Learning rate: 0.001\n",
      "- Model: \n",
      "\n",
      " LSTM(\n",
      "  (LSTMs): ModuleList(\n",
      "    (0): LSTM(722, 500, batch_first=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=500, out_features=354, bias=True)\n",
      "  (embedding): Embedding(2, 5)\n",
      ") \n",
      "\n",
      "Epoch: 1, batch: 415/415, loss: 3.45704\n",
      "Test Error:\n",
      "Acc@1: 11.07%, Acc@5: 25.54%, Acc@10: 34.60%, Acc@20: 47.58% Avg loss: 4.532488\n",
      "\n",
      "Epoch: 2, batch: 415/415, loss: 3.10745\n",
      "Test Error:\n",
      "Acc@1: 14.84%, Acc@5: 32.70%, Acc@10: 42.82%, Acc@20: 54.51% Avg loss: 4.287178\n",
      "\n",
      "Epoch: 3, batch: 415/415, loss: 2.65502\n",
      "Test Error:\n",
      "Acc@1: 13.16%, Acc@5: 34.07%, Acc@10: 44.79%, Acc@20: 56.10% Avg loss: 4.247433\n",
      "\n",
      "Epoch: 4, batch: 415/415, loss: 2.76267\n",
      "Test Error:\n",
      "Acc@1: 14.16%, Acc@5: 35.73%, Acc@10: 46.13%, Acc@20: 57.94% Avg loss: 4.173951\n",
      "\n",
      "Epoch: 5, batch: 415/415, loss: 2.51457\n",
      "Test Error:\n",
      "Acc@1: 13.35%, Acc@5: 37.16%, Acc@10: 48.26%, Acc@20: 60.40% Avg loss: 4.102117\n",
      "\n",
      "Epoch: 6, batch: 415/415, loss: 2.26441\n",
      "Test Error:\n",
      "Acc@1: 19.70%, Acc@5: 41.99%, Acc@10: 51.73%, Acc@20: 63.55% Avg loss: 3.889068\n",
      "\n",
      "Epoch: 7, batch: 415/415, loss: 2.03111\n",
      "Test Error:\n",
      "Acc@1: 20.26%, Acc@5: 42.37%, Acc@10: 52.86%, Acc@20: 64.56% Avg loss: 3.867613\n",
      "\n",
      "Epoch: 8, batch: 415/415, loss: 1.93104\n",
      "Test Error:\n",
      "Acc@1: 18.41%, Acc@5: 41.42%, Acc@10: 51.71%, Acc@20: 63.39% Avg loss: 3.925411\n",
      "\n",
      "Epoch: 9, batch: 415/415, loss: 1.75115\n",
      "Test Error:\n",
      "Acc@1: 20.71%, Acc@5: 43.78%, Acc@10: 53.81%, Acc@20: 65.40% Avg loss: 3.829204\n",
      "\n",
      "Epoch: 10, batch: 415/415, loss: 1.76375\n",
      "Test Error:\n",
      "Acc@1: 18.85%, Acc@5: 42.39%, Acc@10: 53.23%, Acc@20: 65.04% Avg loss: 3.873052\n",
      "\n",
      "Epoch: 11, batch: 415/415, loss: 1.81167\n",
      "Test Error:\n",
      "Acc@1: 19.38%, Acc@5: 43.97%, Acc@10: 54.92%, Acc@20: 66.90% Avg loss: 3.813635\n",
      "\n",
      "Epoch: 12, batch: 415/415, loss: 1.69956\n",
      "Test Error:\n",
      "Acc@1: 20.40%, Acc@5: 43.85%, Acc@10: 54.61%, Acc@20: 66.43% Avg loss: 3.806347\n",
      "\n",
      "Epoch: 13, batch: 415/415, loss: 1.84070\n",
      "Test Error:\n",
      "Acc@1: 20.51%, Acc@5: 45.22%, Acc@10: 55.44%, Acc@20: 66.66% Avg loss: 3.790103\n",
      "\n",
      "Epoch: 14, batch: 415/415, loss: 1.86318\n",
      "Test Error:\n",
      "Acc@1: 19.14%, Acc@5: 44.68%, Acc@10: 55.58%, Acc@20: 66.99% Avg loss: 3.799152\n",
      "\n",
      "Epoch: 15, batch: 415/415, loss: 1.53071\n",
      "Test Error:\n",
      "Acc@1: 22.70%, Acc@5: 47.21%, Acc@10: 57.11%, Acc@20: 67.90% Avg loss: 3.709256\n",
      "\n",
      "Epoch: 16, batch: 415/415, loss: 1.62503\n",
      "Test Error:\n",
      "Acc@1: 21.32%, Acc@5: 46.63%, Acc@10: 57.46%, Acc@20: 69.27% Avg loss: 3.696639\n",
      "\n",
      "Epoch: 17, batch: 415/415, loss: 1.68095\n",
      "Test Error:\n",
      "Acc@1: 21.90%, Acc@5: 46.38%, Acc@10: 56.90%, Acc@20: 68.54% Avg loss: 3.733028\n",
      "\n",
      "Epoch: 18, batch: 415/415, loss: 1.63648\n",
      "Test Error:\n",
      "Acc@1: 22.48%, Acc@5: 46.61%, Acc@10: 57.36%, Acc@20: 68.36% Avg loss: 3.730249\n",
      "\n",
      "Epoch: 19, batch: 415/415, loss: 1.52565\n",
      "Test Error:\n",
      "Acc@1: 21.38%, Acc@5: 46.44%, Acc@10: 56.81%, Acc@20: 68.82% Avg loss: 3.722319\n",
      "\n",
      "Epoch: 20, batch: 415/415, loss: 1.64381\n",
      "Test Error:\n",
      "Acc@1: 23.39%, Acc@5: 47.71%, Acc@10: 58.06%, Acc@20: 69.19% Avg loss: 3.684148\n",
      "\n",
      "Current iteration 2/64\n",
      "- Learning rate: 0.001\n",
      "- Model: \n",
      "\n",
      " LSTM(\n",
      "  (LSTMs): ModuleList(\n",
      "    (0): LSTM(722, 750, batch_first=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=750, out_features=354, bias=True)\n",
      "  (embedding): Embedding(2, 5)\n",
      ") \n",
      "\n",
      "Epoch: 1, batch: 415/415, loss: 3.32136\n",
      "Test Error:\n",
      "Acc@1: 11.30%, Acc@5: 26.23%, Acc@10: 36.31%, Acc@20: 47.97% Avg loss: 4.484626\n",
      "\n",
      "Epoch: 2, batch: 415/415, loss: 2.89997\n",
      "Test Error:\n",
      "Acc@1: 13.80%, Acc@5: 30.54%, Acc@10: 42.06%, Acc@20: 54.56% Avg loss: 4.299266\n",
      "\n",
      "Epoch: 3, batch: 415/415, loss: 2.49285\n",
      "Test Error:\n",
      "Acc@1: 18.07%, Acc@5: 39.08%, Acc@10: 49.72%, Acc@20: 61.67% Avg loss: 4.011805\n",
      "\n",
      "Epoch: 4, batch: 415/415, loss: 2.33126\n",
      "Test Error:\n",
      "Acc@1: 17.34%, Acc@5: 39.02%, Acc@10: 49.66%, Acc@20: 60.53% Avg loss: 4.039070\n",
      "\n",
      "Epoch: 5, batch: 415/415, loss: 2.23095\n",
      "Test Error:\n",
      "Acc@1: 16.50%, Acc@5: 40.37%, Acc@10: 50.94%, Acc@20: 62.92% Avg loss: 3.961862\n",
      "\n",
      "Epoch: 6, batch: 415/415, loss: 2.14301\n",
      "Test Error:\n",
      "Acc@1: 20.05%, Acc@5: 43.94%, Acc@10: 54.52%, Acc@20: 66.35% Avg loss: 3.800061\n",
      "\n",
      "Epoch: 7, batch: 415/415, loss: 2.03132\n",
      "Test Error:\n",
      "Acc@1: 20.62%, Acc@5: 43.21%, Acc@10: 53.62%, Acc@20: 65.38% Avg loss: 3.824634\n",
      "\n",
      "Epoch: 8, batch: 415/415, loss: 1.76117\n",
      "Test Error:\n",
      "Acc@1: 19.87%, Acc@5: 43.57%, Acc@10: 53.84%, Acc@20: 65.19% Avg loss: 3.842053\n",
      "\n",
      "Epoch: 9, batch: 415/415, loss: 1.78286\n",
      "Test Error:\n",
      "Acc@1: 24.05%, Acc@5: 47.62%, Acc@10: 57.83%, Acc@20: 69.16% Avg loss: 3.643731\n",
      "\n",
      "Epoch: 10, batch: 415/415, loss: 1.77114\n",
      "Test Error:\n",
      "Acc@1: 21.11%, Acc@5: 45.82%, Acc@10: 56.68%, Acc@20: 67.94% Avg loss: 3.756686\n",
      "\n",
      "Epoch: 11, batch: 415/415, loss: 1.50213\n",
      "Test Error:\n",
      "Acc@1: 20.61%, Acc@5: 46.00%, Acc@10: 56.94%, Acc@20: 68.76% Avg loss: 3.734551\n",
      "\n",
      "Epoch: 12, batch: 415/415, loss: 1.57269\n",
      "Test Error:\n",
      "Acc@1: 23.12%, Acc@5: 47.82%, Acc@10: 58.49%, Acc@20: 69.73% Avg loss: 3.641658\n",
      "\n",
      "Epoch: 13, batch: 415/415, loss: 1.46262\n",
      "Test Error:\n",
      "Acc@1: 24.18%, Acc@5: 48.52%, Acc@10: 58.84%, Acc@20: 70.39% Avg loss: 3.616735\n",
      "\n",
      "Epoch: 14, batch: 415/415, loss: 1.58839\n",
      "Test Error:\n",
      "Acc@1: 25.07%, Acc@5: 48.88%, Acc@10: 59.23%, Acc@20: 69.81% Avg loss: 3.622219\n",
      "\n",
      "Epoch: 15, batch: 415/415, loss: 1.44470\n",
      "Test Error:\n",
      "Acc@1: 24.91%, Acc@5: 49.52%, Acc@10: 59.99%, Acc@20: 70.65% Avg loss: 3.588326\n",
      "\n",
      "Epoch: 16, batch: 415/415, loss: 1.36707\n",
      "Test Error:\n",
      "Acc@1: 23.52%, Acc@5: 48.39%, Acc@10: 58.86%, Acc@20: 70.10% Avg loss: 3.644958\n",
      "\n",
      "Epoch: 17, batch: 415/415, loss: 1.65356\n",
      "Test Error:\n",
      "Acc@1: 25.48%, Acc@5: 49.82%, Acc@10: 60.47%, Acc@20: 71.42% Avg loss: 3.568903\n",
      "\n",
      "Epoch: 18, batch: 415/415, loss: 1.41983\n",
      "Test Error:\n",
      "Acc@1: 26.26%, Acc@5: 50.39%, Acc@10: 60.25%, Acc@20: 71.29% Avg loss: 3.565273\n",
      "\n",
      "Epoch: 19, batch: 415/415, loss: 1.36837\n",
      "Test Error:\n",
      "Acc@1: 24.39%, Acc@5: 49.78%, Acc@10: 60.37%, Acc@20: 71.86% Avg loss: 3.583031\n",
      "\n",
      "Epoch: 20, batch: 415/415, loss: 1.50137\n",
      "Test Error:\n",
      "Acc@1: 23.91%, Acc@5: 48.50%, Acc@10: 59.59%, Acc@20: 71.01% Avg loss: 3.622531\n",
      "\n",
      "Current iteration 3/64\n",
      "- Learning rate: 0.001\n",
      "- Model: \n",
      "\n",
      " LSTM(\n",
      "  (LSTMs): ModuleList(\n",
      "    (0): LSTM(722, 1000, batch_first=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1000, out_features=354, bias=True)\n",
      "  (embedding): Embedding(2, 5)\n",
      ") \n",
      "\n",
      "Epoch: 1, batch: 415/415, loss: 3.19533\n",
      "Test Error:\n",
      "Acc@1: 12.09%, Acc@5: 30.18%, Acc@10: 39.07%, Acc@20: 50.31% Avg loss: 4.392715\n",
      "\n",
      "Epoch: 2, batch: 415/415, loss: 2.63187\n",
      "Test Error:\n",
      "Acc@1: 13.24%, Acc@5: 33.00%, Acc@10: 44.46%, Acc@20: 56.26% Avg loss: 4.230450\n",
      "\n",
      "Epoch: 3, batch: 415/415, loss: 2.48574\n",
      "Test Error:\n",
      "Acc@1: 17.19%, Acc@5: 38.13%, Acc@10: 48.12%, Acc@20: 59.95% Avg loss: 4.028460\n",
      "\n",
      "Epoch: 4, batch: 415/415, loss: 2.22242\n",
      "Test Error:\n",
      "Acc@1: 19.81%, Acc@5: 41.81%, Acc@10: 52.29%, Acc@20: 64.63% Avg loss: 3.878228\n",
      "\n",
      "Epoch: 5, batch: 415/415, loss: 2.04330\n",
      "Test Error:\n",
      "Acc@1: 18.98%, Acc@5: 41.92%, Acc@10: 52.38%, Acc@20: 64.04% Avg loss: 3.890251\n",
      "\n",
      "Epoch: 6, batch: 415/415, loss: 1.93186\n",
      "Test Error:\n",
      "Acc@1: 18.56%, Acc@5: 41.85%, Acc@10: 52.21%, Acc@20: 64.54% Avg loss: 3.894961\n",
      "\n",
      "Epoch: 7, batch: 415/415, loss: 1.85559\n",
      "Test Error:\n",
      "Acc@1: 19.44%, Acc@5: 42.45%, Acc@10: 52.64%, Acc@20: 63.96% Avg loss: 3.890484\n",
      "\n",
      "Epoch: 8, batch: 415/415, loss: 1.64018\n",
      "Test Error:\n",
      "Acc@1: 19.21%, Acc@5: 44.59%, Acc@10: 55.84%, Acc@20: 67.50% Avg loss: 3.790464\n",
      "\n",
      "Epoch: 9, batch: 415/415, loss: 1.71253\n",
      "Test Error:\n",
      "Acc@1: 22.08%, Acc@5: 46.95%, Acc@10: 57.18%, Acc@20: 68.24% Avg loss: 3.707672\n",
      "\n",
      "Epoch: 10, batch: 150/415, loss: 1.71205\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "current = 0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "full_results = []\n",
    "\n",
    "try:            \n",
    "    for learning_rate in [1e-1, 1e-2, 1e-3, 1e-4][2:]:\n",
    "        for num_layers in [1, 5, 10]:\n",
    "            for hidden_size in [500, 1000][1:]:\n",
    "\n",
    "                lstm = LSTM(num_classes=num_classes,\n",
    "                            input_size=num_features,\n",
    "                            num_layers=num_layers,\n",
    "                            hidden_size=hidden_size,\n",
    "                            embedding_size=5,\n",
    "                            skills=skills, \n",
    "                            certs=certs,\n",
    "                            w2v=w2v,\n",
    "                            candidate_lengths=candidate_lens,\n",
    "                            max_len=max_len)\n",
    "\n",
    "                lstm = lstm.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "                print(f\"Current iteration {current}/{4**3}\\n- Learning rate: {learning_rate}\\n- Model: \\n\\n\", lstm, \"\\n\")\n",
    "\n",
    "                # Store results of current configuration\n",
    "                outcome = train_loop(lstm, trainloader, valloader, optimizer, criterion, num_epochs)\n",
    "                outcome[\"lr\"] = [learning_rate] * num_epochs\n",
    "                outcome[\"Number of layers\"] = [num_layers] * num_epochs\n",
    "                outcome[\"Nodes per layer\"] = [hidden_size] * num_epochs\n",
    "\n",
    "                full_results.append(outcome)\n",
    "\n",
    "                current += 1\n",
    "            break\n",
    "\n",
    "        # We ignore LR for now\n",
    "        break\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = defaultdict(list)\n",
    "\n",
    "for res in full_results:\n",
    "    for k, v in res.items():\n",
    "        merge_results[k].extend(v)\n",
    "        \n",
    "total = pd.DataFrame(merge_results).set_index([\"lr\", \"Number of layers\", \"Nodes per layer\", \"Epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "659cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.203125\n",
      "Counter({331: 49, 343: 21, 348: 13, 176: 11, 163: 10, 184: 10, 350: 9, 300: 9, 335: 8, 341: 8, 322: 8, 342: 7, 351: 7, 179: 7, 196: 7, 155: 6, 333: 6, 231: 6, 193: 6, 347: 6, 173: 6, 321: 6, 344: 6, 213: 5, 117: 5, 214: 5, 177: 5, 327: 5, 273: 5, 326: 5, 264: 4, 203: 4, 2: 4, 238: 4, 221: 4, 98: 4, 325: 4, 79: 4, 338: 4, 206: 3, 289: 3, 200: 3, 71: 3, 271: 3, 187: 3, 83: 3, 143: 3, 156: 3, 232: 3, 137: 3, 305: 3, 207: 3, 175: 3, 192: 3, 129: 3, 24: 3, 190: 3, 168: 3, 307: 3, 153: 3, 61: 3, 46: 3, 261: 3, 144: 3, 194: 3, 182: 3, 208: 2, 114: 2, 107: 2, 4: 2, 6: 2, 224: 2, 239: 2, 254: 2, 150: 2, 145: 2, 290: 2, 62: 2, 164: 2, 186: 2, 336: 2, 95: 2, 77: 2, 345: 2, 92: 2, 103: 2, 269: 2, 222: 2, 274: 2, 188: 2, 252: 2, 174: 2, 139: 2, 178: 2, 74: 2, 233: 2, 189: 2, 240: 2, 235: 2, 265: 2, 70: 2, 91: 2, 282: 2, 97: 2, 73: 1, 283: 1, 89: 1, 136: 1, 162: 1, 260: 1, 110: 1, 205: 1, 140: 1, 181: 1, 118: 1, 5: 1, 285: 1, 180: 1, 72: 1, 201: 1, 12: 1, 141: 1, 352: 1, 195: 1, 14: 1, 9: 1, 211: 1, 288: 1, 51: 1, 23: 1, 320: 1, 34: 1, 293: 1, 68: 1, 166: 1, 33: 1, 251: 1, 242: 1, 262: 1, 318: 1, 319: 1, 198: 1, 152: 1, 13: 1, 149: 1, 84: 1, 353: 1, 204: 1, 159: 1, 60: 1, 80: 1, 126: 1, 15: 1, 63: 1, 266: 1, 49: 1})\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for candidate, career, job in valloader:\n",
    "        candidate, career, job = candidate.to(device), career.to(device), job.to(device)\n",
    "        pred = lstm(candidate, career)\n",
    "        \n",
    "        print((pred.argmax(1) == job).type(torch.float).mean().item())        \n",
    "        print(Counter(pred.argmax(1).tolist()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76663c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(2, 4)\n",
    "inpt = torch.Tensor([[1, 1, 1],\n",
    "                     [0, 1, 0]]).type(torch.LongTensor)\n",
    "\n",
    "emb(inpt).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3802da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
