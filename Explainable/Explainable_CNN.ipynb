{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53ea606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: grad-cam in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: pytorch-gradcam in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: torchvision==0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.0+cu92)\n",
      "Requirement already satisfied: pillow<7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (6.2.2)\n",
      "Requirement already satisfied: torch==1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision==0.4.0) (1.2.0+cu92)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision==0.4.0) (1.19.5)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-gradcam) (4.5.1.48)\n"
     ]
    }
   ],
   "source": [
    "!pip install grad-cam pytorch-gradcam torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html \"pillow<7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6dbacb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_grad_cam.utils.model_targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8bc3dd4db5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeightedRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_grad_cam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mScoreCAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradCAMPlusPlus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAblationCAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXGradCAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEigenCAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_grad_cam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_targets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierOutputTarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_grad_cam.utils.model_targets'"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/utils\")\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/config\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9baed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = pd.read_csv(\"../Data/skills_one-hot.csv\").set_index(\"candidate_id\")\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = dict(zip(skills.index, skills.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "certs = pd.read_csv(\"../Data/candidate_certificates_one-hot.csv\").set_index(\"candidate_id\")\n",
    "certs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "certs = dict(zip(certs.index, certs.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77877876",
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses = pd.read_csv(\"../Data/licenses_one-hot.csv\").set_index(\"candidate_id\")\n",
    "licenses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307656f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses = dict(zip(licenses.index, licenses.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.read_csv(\"../Data/languages_one-hot.csv\").set_index(\"candidate_id\")\n",
    "languages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = dict(zip(languages.index, languages.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c308e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = pd.read_csv(\"../Data/addresses_one-hot.csv\").set_index(\"candidate_id\")\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047bda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = dict(zip(addresses.index, addresses.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410810e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = json.load(open(\"../Data/embeddings.json\"))\n",
    "# Convert to ints\n",
    "w2v = {int(k):{int(k2):v2 for k2, v2 in v.items()} for k, v in w2v.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33130f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(\"../Data/df_pred_ext.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[\"time_between\"] = (df_pred[\"time_between\"] - df_pred[\"time_between\"].mean()) / df_pred[\"time_between\"].std()\n",
    "df_pred[\"time_spent\"] = (df_pred[\"time_spent\"] - df_pred[\"time_spent\"].mean()) / df_pred[\"time_spent\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "career_paths = df_pred.groupby(\"candidate_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe955a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df_pred[\"isco_code4\"].unique())\n",
    "num_features = len(career_paths.mean().columns)\n",
    "num_classes, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe66e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_career_duration = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2d-arrays, grabbing the last 25 jobs of each candidate and getting rid of candidate_ids as values\n",
    "career_paths = career_paths.progress_apply(lambda x: x.values[-(maximum_career_duration + 1):,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop careers that are only 1 job long\n",
    "career_lens = career_paths.apply(len)\n",
    "career_paths = career_paths.loc[(career_lens > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "career_paths = career_paths.loc[career_paths.apply(lambda x: x[-1][-1] != x[-2][-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "career_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "candidate_lens = defaultdict(int)\n",
    "\n",
    "# max_skills = len([col for col in df_pred if \"skill_\" in col])\n",
    "\n",
    "for idx, career in zip(career_paths.index, career_paths.values):\n",
    "    label = career[-1, -1]\n",
    "    \n",
    "    if not np.isnan(label):\n",
    "        candidate_lens[idx] = len(career) - 1\n",
    "        \n",
    "        idxs.append(idx)\n",
    "        x.append(career[:-1].reshape(len(career) - 1, num_features))\n",
    "        y.append(label)\n",
    "\n",
    "idxs = np.array(idxs)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill = np.zeros([len(x), len(max(x, key = lambda x: len(x))), num_features])\n",
    "\n",
    "for i,j in enumerate(x):\n",
    "    if len(j):\n",
    "        to_fill[i][-len(j):] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(max(x, key = lambda x: len(x)))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a32bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pred\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39449c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c406b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_fill), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill = to_fill[:50000]\n",
    "y = y[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a84c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "split = 0.8\n",
    "random.seed(42)\n",
    "\n",
    "training = np.array(random.sample(range(len(to_fill)), int(split * len(to_fill))))\n",
    "test = np.array(list(set(range(len(to_fill))) - set(training)))\n",
    "\n",
    "train_indices, val_indices = idxs[training], idxs[test]\n",
    "X_train, X_val = to_fill[training], to_fill[test]\n",
    "y_train, y_val = y[training].astype(int), y[test].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "counts = np.bincount(y_train) + 1\n",
    "\n",
    "# TODO: Change so that common classes get punished slightly more\n",
    "labels_weights = 2. / (0.5 * np.sqrt(counts))\n",
    "weights = labels_weights[y_train]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# Create dataloaders\n",
    "train_data = TensorDataset(torch.Tensor(train_indices), \n",
    "                           torch.Tensor(X_train), \n",
    "                           torch.Tensor(y_train).type(torch.LongTensor))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=512, sampler=sampler)\n",
    "\n",
    "val_data = TensorDataset(torch.Tensor(val_indices),\n",
    "                         torch.Tensor(X_val),\n",
    "                         torch.Tensor(y_val).type(torch.LongTensor))\n",
    "\n",
    "valloader = DataLoader(val_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature maps per time step\n",
    "# N = features (each feature is a 'time series' in the words of the authors), T = time steps\n",
    "\n",
    "k1 = 25\n",
    "f_maps = 100\n",
    "N = 9\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels=1,\n",
    "                   out_channels=f_maps,\n",
    "                   kernel_size=(1, k1),\n",
    "                   stride=(1, 2),\n",
    "                   padding=(0, k1 // 2))\n",
    "\n",
    "_1x1 = nn.Conv2d(in_channels=f_maps, \n",
    "                 out_channels=1, \n",
    "                 kernel_size=(1, 1), # Maybe (100, 1, 1)?\n",
    "                 stride=1)\n",
    "\n",
    "# Feature map for all features\n",
    "# 84 = n_features\n",
    "conv1d = nn.Conv1d(in_channels=1,\n",
    "                   out_channels=1, \n",
    "                   kernel_size=(N, k1 // 2),\n",
    "                   padding=(N // 2, k1 // 2 // 2 // 2)) # lol\n",
    "\n",
    "for c, i, j in trainloader:\n",
    "    \n",
    "    i = i.unsqueeze(-1)\n",
    "    i = i.transpose(1, 3)\n",
    "    print(i.shape)\n",
    "    x = conv2d(i) \n",
    "    print(x.shape)\n",
    "    x = _1x1(x)\n",
    "    print(x.shape)\n",
    "    x = conv1d(x)\n",
    "    \n",
    "    x = x.flatten(start_dim=1)\n",
    "    \n",
    "    print(x.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, conv_size, \n",
    "                 skills, certs, licenses, languages, \n",
    "                 addresses, w2v, candidate_lengths, max_len, \n",
    "                 skill_embedding_size=50, certs_embedding_size=20,\n",
    "                 license_embedding_size=3, language_embedding_size=10,\n",
    "                 address_embedding_size=25, function_embedding_size=50, \n",
    "                 isco4_embedding_size=25, education_embedding_size=3, \n",
    "                 isco_level_embedding_size=3, company_embedding_size=50):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "              \n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size + 300\n",
    "        self.conv_size = conv_size\n",
    "        \n",
    "        # Static embeddings: skills, certificates, licenses, languages\n",
    "        self.skill_embedding = nn.Linear(317, skill_embedding_size, bias=False)\n",
    "        self.skill_embedding.weight.data = torch.randn_like(self.skill_embedding.weight) \n",
    "        \n",
    "        self.certs_embedding = nn.Linear(98, certs_embedding_size, bias=False)\n",
    "        self.certs_embedding.weight.data = torch.randn_like(self.certs_embedding.weight) \n",
    "        \n",
    "        self.license_embedding = nn.Linear(8, license_embedding_size, bias=False)\n",
    "        self.license_embedding.weight.data = torch.randn_like(self.license_embedding.weight) \n",
    "        \n",
    "        self.language_embedding = nn.Linear(23, language_embedding_size, bias=False)\n",
    "        self.language_embedding.weight.data = torch.randn_like(self.language_embedding.weight) \n",
    "        \n",
    "        # Address embedding\n",
    "        self.address_embedding = nn.Embedding(4757, address_embedding_size)       \n",
    "        \n",
    "        # Categorical feature embeddings\n",
    "        self.function_embedding = nn.Embedding(2992, function_embedding_size)\n",
    "        self.isco_code_embedding = nn.Embedding(num_classes, isco4_embedding_size)\n",
    "        self.company_embedding = nn.Embedding(441153, company_embedding_size)\n",
    "        self.source_embedding = nn.Embedding(2, 1)\n",
    "        self.education_embedding = nn.Embedding(6, education_embedding_size)\n",
    "        self.isco_level_embedding = nn.Embedding(5, isco_level_embedding_size)\n",
    "        \n",
    "        # Actual model\n",
    "        \n",
    "        # Feature maps per time step\n",
    "#         self.conv2d = nn.Conv2d(in_channels=1,\n",
    "#                                 out_channels=25,\n",
    "#                                 kernel_size=(84, 1),\n",
    "#                                 stride=(1, 2))\n",
    "        \n",
    "#         self._1x1 = nn.Conv3d(in_channels=, \n",
    "#                               out_channels=1, \n",
    "#                               kernel_size=(1, 1, 25))\n",
    "        \n",
    "#         # Feature map for all features\n",
    "#         # 84 = n_features\n",
    "#         self.conv1d = nn.Conv1d(in_channels=1,\n",
    "#                                 out_channels=max_len, \n",
    "#                                 kernel_size=(84, max_len))\n",
    "        \n",
    "        \n",
    "        k1 = max_len\n",
    "        f_maps = 20\n",
    "        N = 84\n",
    "        \n",
    "        # TODO: more than 1 conv2d\n",
    "        self.conv2d = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=f_maps,\n",
    "                                kernel_size=(1, k1),\n",
    "                                stride=(1, 2),\n",
    "                                padding=(0, k1 // 2))\n",
    "     \n",
    "        self._1x1 = nn.Conv2d(in_channels=f_maps, \n",
    "                              out_channels=1, \n",
    "                              kernel_size=(1, 1), # Maybe (100, 1, 1)?\n",
    "                              stride=1)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=1,\n",
    "                                out_channels=1, \n",
    "                                kernel_size=(N, k1 // 2),\n",
    "                                padding=(N // 2, k1 // 2 // 2 // 2)) # lol\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 109 = n_feautres + n_time_steps\n",
    "        self.fc = nn.Linear(1088, num_classes)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        \n",
    "        # Skill lookup\n",
    "        self.skill_keys = set(skills.keys())\n",
    "        self.skills = np.vectorize(skills.get)\n",
    "        \n",
    "        # Certificate lookup\n",
    "        self.certs_keys = set(certs.keys())\n",
    "        self.certs = np.vectorize(certs.get)\n",
    "        \n",
    "        # License lookup\n",
    "        self.license_keys = set(licenses.keys())\n",
    "        self.licenses = np.vectorize(licenses.get)\n",
    "        \n",
    "        # Language lookup\n",
    "        self.langs_keys = set(languages.keys())\n",
    "        self.langs = np.vectorize(languages.get)\n",
    "        \n",
    "        # Address lookup\n",
    "        self.address_keys = set(addresses.keys())\n",
    "        self.adds = np.vectorize(addresses.get)\n",
    "        \n",
    "        # w2v lookup\n",
    "        self.w2v_keys = set(w2v.keys())\n",
    "        self.w2v = w2v\n",
    "        \n",
    "        # Career durations\n",
    "        self.candidate_lengths = candidate_lengths\n",
    "        self.max_len = max_len        \n",
    "                \n",
    "    def static_lookup(self, candidate):\n",
    "        \"\"\"Looks up a candidate's static features (skills, certificates)\"\"\"\n",
    "         # Look up skills            \n",
    "        if candidate.item() in self.skill_keys:\n",
    "            skill_list = torch.LongTensor(self.skills(candidate.item())).to(device)\n",
    "        else:\n",
    "            skill_list = torch.LongTensor([0] * 317).to(device)\n",
    "\n",
    "        # Look up certificates\n",
    "        if candidate.item() in self.certs_keys:\n",
    "            certs_list = torch.LongTensor(self.certs(candidate.item())).to(device)\n",
    "        else:\n",
    "            certs_list = torch.LongTensor([0] * 98).to(device)\n",
    "            \n",
    "        # Look up certificates\n",
    "        if candidate.item() in self.license_keys:\n",
    "            license_list = torch.LongTensor(self.licenses(candidate.item())).to(device)\n",
    "        else:\n",
    "            license_list = torch.LongTensor([0] * 8).to(device)\n",
    "        \n",
    "        # Look up certificates\n",
    "        if candidate.item() in self.langs_keys:\n",
    "            langs_list = torch.LongTensor(self.langs(candidate.item())).to(device)\n",
    "        else:\n",
    "            langs_list = torch.LongTensor([0] * 23).to(device)\n",
    "            \n",
    "        # Look up address\n",
    "        if candidate.item() in self.address_keys:\n",
    "            address = torch.LongTensor(self.adds(candidate.item())).to(device)\n",
    "        else:\n",
    "            address = torch.LongTensor([0]).to(device)\n",
    "            \n",
    "        return skill_list, certs_list, license_list, langs_list, address\n",
    "    \n",
    "    def w2v_lookup(self, candidate, career_duration):\n",
    "        \"\"\"Finds a candidate's CVs and converts them to a tensor of length career_duration\"\"\"\n",
    "            \n",
    "        # Look for cvs\n",
    "        if candidate.item() in self.w2v_keys:\n",
    "            cvs = self.w2v[candidate.item()]\n",
    "                \n",
    "            storage = []\n",
    "\n",
    "             # If a candidate only has one CV, proceed as normal\n",
    "            if len(cvs.keys()) == 1:\n",
    "                w2v_list = torch.LongTensor(cvs[0]).to(device)\n",
    "                w2v_list = torch.stack([w2v_list] * career_duration)\n",
    "            else: # Otherwise, stack them accordingly\n",
    "                ks = np.array(list(cvs.keys()))\n",
    "                # Due to clipping, some careers are longer than max_len\n",
    "                ks = np.array([k for k in ks if k <= self.max_len])\n",
    "\n",
    "                # Find how many time steps (rows) each CV lasted\n",
    "                durations = [ks[i+1] - ks[i]\n",
    "                             if i < (len(ks) - 1) \n",
    "                             else career_duration - ks[i]\n",
    "                             for i in range(len(ks))]\n",
    "\n",
    "                embed_values = list(cvs.values())\n",
    "\n",
    "                # When the CV got updated on the last timestep, aka our test value\n",
    "                # Remove it from the list of durations, as it should be ignored\n",
    "                if durations[-1] == 0: \n",
    "                    durations.pop()\n",
    "                if durations[-1] == -1: # Sometimes contains -1 --> last location > (career duration)?\n",
    "                    durations.pop()\n",
    "                    durations[-1] -= 1\n",
    "                    # In case the last one should be ignored completely\n",
    "                    if durations[-1] == 0:\n",
    "                        durations.pop()\n",
    "\n",
    "                # Create Tensor(s)\n",
    "                if durations:\n",
    "                    for i, duration in enumerate(durations):\n",
    "                        storage.append(torch.stack([torch.Tensor(embed_values[i])] * duration, dim=0))\n",
    "                else:\n",
    "                    w2v_list = torch.LongTensor(cvs[0]).to(device)\n",
    "\n",
    "                # Combine stored tensors into a single tensor\n",
    "                w2v_list = torch.cat((storage)).type(torch.LongTensor).to(device)\n",
    "        else:\n",
    "            w2v_list = torch.LongTensor([0] * 300).to(device)\n",
    "            w2v_list = torch.stack([w2v_list] * career_duration)\n",
    "\n",
    "        return w2v_list\n",
    " \n",
    "    def forward(self, x):               \n",
    "        # Default width of a row (filled with 0s)\n",
    "        feature_width = torch.Tensor([0] * 408).type(torch.LongTensor).to(device)\n",
    "        \n",
    "        candidate_features = []\n",
    "                \n",
    "#         candidate, x = x\n",
    "            \n",
    "#         # For each candidate in the current batch\n",
    "#         for c in candidate:\n",
    "#             # Get career duration\n",
    "#             career_duration = self.candidate_lengths[c.item()]            \n",
    "            \n",
    "#             # Get skills and certificates\n",
    "#             skill_list, certs_list, license_list, langs_list, address = self.static_lookup(c)\n",
    "            \n",
    "#             # Get CV embeddings\n",
    "#             w2v_list = self.w2v_lookup(c, career_duration)\n",
    "\n",
    "#             # Only create zeros if needed (e.g. less than max_len career duration)\n",
    "#             if (self.max_len - career_duration) > 0:\n",
    "#                 zeros = torch.stack([feature_width] * (self.max_len - career_duration))                \n",
    "#             else: # Reset zeros to prevent shape mismatch\n",
    "#                 zeros = torch.LongTensor([]).to(device)\n",
    "                \n",
    "#             # Embed every static feature\n",
    "#             skill_list, certs_list, license_list, langs_list = [self.skill_embedding(skill_list.type(torch.FloatTensor).to(device)),\n",
    "#                                                                 self.certs_embedding(certs_list.type(torch.FloatTensor).to(device)),\n",
    "#                                                                 self.license_embedding(license_list.type(torch.FloatTensor).to(device)),\n",
    "#                                                                 self.language_embedding(langs_list.type(torch.FloatTensor).to(device))]\n",
    "                \n",
    "#             # Combine static features\n",
    "#             static_features = torch.cat([skill_list, certs_list, \n",
    "#                                          license_list, langs_list], dim=-1).type(torch.FloatTensor).to(device)\n",
    "            \n",
    "#             # Embed address\n",
    "#             address_emb = self.address_embedding(address)[0]\n",
    "                        \n",
    "#             # Broadcast and add static features\n",
    "#             static_features = torch.stack([static_features] * career_duration).type(torch.LongTensor).to(device)\n",
    "#             address_emb = torch.stack([address_emb] * career_duration).type(torch.LongTensor).to(device)\n",
    "            \n",
    "#             # Combine w2v, static features, and address\n",
    "#             full_features = torch.cat([w2v_list, static_features, address_emb], dim=1)\n",
    "                                    \n",
    "#             # Broadcast CV, static, and address to the correct length\n",
    "#             full_features = torch.cat([zeros, full_features], dim=0)\n",
    "                    \n",
    "#             # Store result\n",
    "#             candidate_features.append(full_features)\n",
    "                                \n",
    "#         # Convert list of tensors to actual tensor\n",
    "#         additional_features = torch.stack((candidate_features)).type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        # isco_functie_niveau, education, function_id, isco_code4\n",
    "        isco_level, source, education, company_name, function_id, isco_code = [x[:,:,-6],\n",
    "                                                                               x[:,:,-5],\n",
    "                                                                               x[:,:,-4],\n",
    "                                                                               x[:,:,-3],\n",
    "                                                                               x[:,:,-2],\n",
    "                                                                               x[:,:,-1]]\n",
    "        \n",
    "        x = x[:,:,:-6].to(device)\n",
    "\n",
    "        isco_level, source, education, company_name, function_id, isco_code  = [self.isco_level_embedding(isco_level.type(torch.LongTensor).to(device)),\n",
    "                                                                                self.source_embedding(source.type(torch.LongTensor).to(device)),\n",
    "                                                                                self.education_embedding(education.type(torch.LongTensor).to(device)),\n",
    "                                                                                self.company_embedding(company_name.type(torch.LongTensor).to(device)),\n",
    "                                                                                self.function_embedding(function_id.type(torch.LongTensor).to(device)),\n",
    "                                                                                self.isco_code_embedding(isco_code.type(torch.LongTensor).to(device))]\n",
    "                \n",
    "        # Add features\n",
    "        x = torch.cat([x, isco_level, source, education, company_name, function_id, isco_code], dim=2)\n",
    "\n",
    "        # Reshape to allow conv2D\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # K time steps, 1 feature at a time\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        # Reduce to 1 feature map\n",
    "        x = self._1x1(x)\n",
    "        \n",
    "        # Patterns across all features\n",
    "        x = self.conv1d(x)\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        \n",
    "        out = self.fc(x)\n",
    "               \n",
    "        # softmax\n",
    "        out = self.softmax(out)                        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, trainloader, valloader, optimizer, scheduler, criterion, num_epochs):\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    passed = [0]\n",
    "    training_losses = [6]\n",
    "    test_losses = [6]\n",
    "    accuracy = [0]\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"Epoch starting at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        training_loss = 0\n",
    "        \n",
    "        for i, (candidate, career, job) in enumerate(trainloader):\n",
    "            \n",
    "            candidate, career, job = candidate.to(device), career.to(device), job.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(career)\n",
    "                        \n",
    "            # obtain the loss function\n",
    "            loss = criterion(outputs, job)\n",
    "            loss = loss.mean()           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "            print(\"Epoch: %d, batch: %d/%d, loss: %1.5f\" % (epoch + 1, i + 1, len(trainloader), loss.item()), end=\"\\r\")\n",
    "               \n",
    "        training_loss /= len(trainloader)\n",
    "                \n",
    "        stats = test_loop(valloader, model, criterion)\n",
    "        results[\"Epoch\"].append(epoch + 1)\n",
    "        results[\"Acc@1\"].append(stats[0])\n",
    "        results[\"Acc@5\"].append(stats[1])\n",
    "        results[\"Acc@10\"].append(stats[2])\n",
    "        results[\"Acc@20\"].append(stats[3])\n",
    "        results[\"test_loss\"].append(stats[4])\n",
    "        results[\"training_loss\"].append(training_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch duration: {int((time.time() - start) // 60)}:{int((time.time() - start) % 60):02d}\\n\")\n",
    "        \n",
    "        passed.append(epoch + 1)\n",
    "        training_losses.append(training_loss)\n",
    "        test_losses.append(stats[4])\n",
    "        accuracy.append(stats[0])\n",
    "        \n",
    "        plt.plot(passed, training_losses, label=\"Training Loss\")\n",
    "        plt.plot(passed, test_losses, label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "                \n",
    "    return results\n",
    "        \n",
    "def test_loop(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, acc1, acc5, acc10, acc20 = 0, 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for candidate, career, job in dataloader:\n",
    "            candidate, career, job = candidate.to(device), career.to(device), job.to(device)\n",
    "            pred = model(career)\n",
    "            \n",
    "            test_loss += criterion(pred, job).mean().item()\n",
    "            acc1 += (pred.argmax(1) == job).type(torch.float).sum().item()\n",
    "            \n",
    "            sorted_preds = torch.argsort(pred, 1, descending=True)\n",
    "            \n",
    "            at5 = []\n",
    "            at10 = []\n",
    "            at20 = []\n",
    "            \n",
    "            for answer, predictions in zip(job, sorted_preds):\n",
    "                at5.append(answer.item() in predictions[:5])\n",
    "                at10.append(answer.item() in predictions[:10])\n",
    "                at20.append(answer.item() in predictions[:20])\n",
    "            \n",
    "            acc5 += np.sum(at5)\n",
    "            acc10 += np.sum(at10)\n",
    "            acc20 += np.sum(at20)\n",
    "            \n",
    "#         a = weights[0].cpu().detach().numpy().mean(axis=0)            \n",
    "#         plt.plot(a, label=\"average\")\n",
    "#         plt.plot(weights[0][np.random.choice(range(len(weights[0])))].cpu().detach().numpy(), label=\"random example\")\n",
    "#         plt.xlabel(\"Career step\")\n",
    "#         plt.ylabel(\"Attention weight\")\n",
    "#         plt.show()\n",
    " \n",
    "            \n",
    "    # print(\"\\nValidation:\", Counter(np.array(pred.argmax(1).cpu())))\n",
    "    test_loss /= num_batches\n",
    "    acc1 /= size\n",
    "    acc5 /= size\n",
    "    acc10 /= size\n",
    "    acc20 /= size\n",
    "    print(f\"\\nTest Error:\")\n",
    "    print(f\"Acc@1: {(100*acc1):>0.2f}%, Acc@5: {100*acc5:>0.2f}%, \" +\\\n",
    "          f\"Acc@10: {100*acc10:>0.2f}%, Acc@20: {100*acc20:>0.2f}% Avg loss: {test_loss:>8f}\")\n",
    "    \n",
    "    return acc1, acc5, acc10, acc20, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb23062",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19114ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "current = 0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "full_results = []\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4][2:]\n",
    "num_layers_values = [1, 5, 10]\n",
    "conv_sizes = [24, 48, 64, 128][1:]\n",
    "\n",
    "try:            \n",
    "    for learning_rate in learning_rates:\n",
    "        for conv_size in conv_sizes:\n",
    "\n",
    "            cnn = CNN(num_classes=num_classes,\n",
    "                      input_size=num_features,\n",
    "                      conv_size=conv_size,\n",
    "                      skills=skills, \n",
    "                      certs=certs,\n",
    "                      licenses=licenses,\n",
    "                      languages=languages,\n",
    "                      addresses=addresses,\n",
    "                      w2v=w2v,\n",
    "                      address_embedding_size=25,\n",
    "                      candidate_lengths=candidate_lens,\n",
    "                      max_len=max_len)\n",
    "\n",
    "            cnn = cnn.to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "            print(f\"Current iteration {current}/{len(learning_rates) * len(num_layers_values) * len(conv_sizes)}\")\n",
    "            print(f\"- Initial learning rate: {learning_rate}\\n- Model: \\n\\n\", cnn, \"\\n\")\n",
    "\n",
    "            # Store results of current configuration\n",
    "            outcome = train_loop(cnn, trainloader, valloader, optimizer, scheduler, criterion, num_epochs)\n",
    "            outcome[\"lr\"] = [learning_rate] * num_epochs\n",
    "            outcome[\"Convolution size\"] = [conv_size] * num_epochs\n",
    "\n",
    "            full_results.append(outcome)\n",
    "\n",
    "            current += 1\n",
    "            \n",
    "            break\n",
    "\n",
    "        # We ignore LR for now\n",
    "        break\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = defaultdict(list)\n",
    "\n",
    "for res in full_results:\n",
    "    for k, v in res.items():\n",
    "        merge_results[k].extend(v)\n",
    "        \n",
    "total = pd.DataFrame(merge_results).set_index([\"lr\", \"Convolution size\", \"Epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for candidate, career, job in valloader:\n",
    "        candidate, career, job = candidate.to(device), career.to(device), job.to(device)\n",
    "        pred = cnn(career)\n",
    "        \n",
    "        print(\"Batch accuracy:\", (pred.argmax(1) == job).type(torch.float).mean().item())        \n",
    "        a = pd.Series(Counter(job.tolist()))\n",
    "        a.sort_index().plot(kind=\"area\", label=\"Ground truth\")\n",
    "        \n",
    "        b = pd.Series(Counter(pred.argmax(1).tolist()))\n",
    "        b.sort_index().plot(kind=\"area\", label=\"predicted\")\n",
    "        plt.xlabel(\"isco_code4\")\n",
    "        plt.ylabel(\"number of occurences\")\n",
    "        plt.legend()\n",
    "        \n",
    "        # Check how often the model predicted the previous job + compare to baseline performance\n",
    "        previous_job = torch.LongTensor(career_paths.loc[candidate.cpu()].apply(lambda x: x[-2][-1]).values).to(device)\n",
    "        print(\"Previous-job baseline accuracy:\", (job == previous_job).cpu().numpy().mean())\n",
    "        print(\"Fraction of previous job predictions:\", (pred.argmax(1) == previous_job).cpu().numpy().mean())\n",
    "        \n",
    "        plt.show()        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = cnn._1x1\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=cnn, target_layer=target_layers, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(240, 10, n=9, as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53740a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = []\n",
    "\n",
    "for c, i, j in valloader:\n",
    "    input_tensor = i.to(device)\n",
    "    a = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(353)])\n",
    "    overall.append(a.mean(axis=0).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.array(overall).sum(axis=0)[:,::-1], cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1fb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214bb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
