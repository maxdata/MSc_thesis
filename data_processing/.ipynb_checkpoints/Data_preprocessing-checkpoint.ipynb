{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/utils\")\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/config\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import s3fs\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "# import sktime\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from redshift import get_from_redshift\n",
    "from datetime import datetime\n",
    "from config import Config  \n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm \n",
    "from itertools import zip_longest\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12404db5",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7001017",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_path = os.path.join(\"/home/ec2-user/SageMaker/data-science-development/talent_recommender/daily_snapshots\",\n",
    "                        Config.query_candidate_work_experience) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24586326",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_work_experience = get_from_redshift(sql_path, Config.redshift_creds)\n",
    "candidate_work_experience.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_path = os.path.join(\"/home/ec2-user/SageMaker/data-science-development/talent_recommender/daily_snapshots\",\n",
    "                        Config.query_candidate_education) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eab70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_education = get_from_redshift(sql_path, Config.redshift_creds)\n",
    "candidate_education.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_path = os.path.join(\"/home/ec2-user/SageMaker/data-science-development/talent_recommender/daily_snapshots\",\n",
    "                        Config.query_candidate_skills) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5458a",
   "metadata": {},
   "source": [
    "#### What's date start here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_skill = get_from_redshift(sql_path, Config.redshift_creds)\n",
    "candidate_skill.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03020f",
   "metadata": {},
   "source": [
    "# Skill reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c33388",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_to_idx = {v : i for i, v in enumerate(sorted(candidate_skill[\"skill_id\"].astype(float).unique()))}\n",
    "idx_to_skill = {i : v for i, v in skill_to_idx.items()}\n",
    "candidate_skill[\"skill_id\"] = candidate_skill[\"skill_id\"].replace(skill_to_idx)\n",
    "skills_dict = candidate_skill.groupby(\"candidate_id\")[\"skill_id\"].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do this in a way that doesn't require 70gb of memory lol\n",
    "# Convert lists of skills to dataframe of skills per candidate\n",
    "skills_ct = pd.crosstab(candidate_skill[\"candidate_id\"], \n",
    "                        candidate_skill[\"skill_id\"])\n",
    "\n",
    "# # skills = skills_ct.where(skills_ct != 1, skills_ct.columns.to_series(), axis=1)\n",
    "skills_ct.columns = [f\"skill_{i}\" for i in skills_ct.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40310c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_ct.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1237e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills_ct.to_csv(\"skills_one-hot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cfd1d",
   "metadata": {},
   "source": [
    "# Merging and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44347b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, table = \"work_experience\"):\n",
    "    \n",
    "    if table == \"work_experience\":\n",
    "        start, end = \"date_start_job\", \"date_end_job\"\n",
    "    elif table == \"education\":\n",
    "        start, end = \"date_start\", \"date_end\"\n",
    "    else:\n",
    "        return NotImplemented\n",
    "    \n",
    "    # Drop nonsense data\n",
    "    df = df[df[start] >= dt.datetime(1800, 1, 1)]\n",
    "    df = df[df[end] <= dt.datetime(2100, 1, 1)]\n",
    "\n",
    "    # Convert datetime to date\n",
    "    df[start] = pd.to_datetime(df[start]).dt.date\n",
    "    df[end] = pd.to_datetime(df[end]).dt.date\n",
    "    \n",
    "    return df\n",
    "        \n",
    "candidate_work_experience = clean_df(candidate_work_experience)\n",
    "candidate_education = clean_df(candidate_education, table = \"education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_work_experience[\"time_spent\"] = (candidate_work_experience[\"date_end_job\"] - \n",
    "                                           candidate_work_experience[\"date_start_job\"]).dt.days.astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_educations(df_work, df_education):\n",
    "\n",
    "    # Merge career data with education levels\n",
    "    career_education = pd.merge(df_work, \n",
    "                                df_education, \n",
    "                                on = \"candidate_id\", \n",
    "                                how = \"left\")[[\"candidate_id\",\n",
    "                                               \"date_start_job\",\n",
    "                                               \"date_end_job\",\n",
    "                                               \"time_spent\",\n",
    "                                               \"education_level\",\n",
    "                                               \"date_start\",\n",
    "                                               \"date_end\",\n",
    "                                               \"passed\"]]\n",
    "    \n",
    "    # Filter out education that were not passed (yet) at the time of starting a job\n",
    "    passed = career_education[(career_education[\"date_start_job\"] >= career_education[\"date_end\"]) & \n",
    "                              (career_education[\"passed\"] == 1)]\n",
    "    \n",
    "    # Only store the highest education level reached at the start of each job\n",
    "    education_through_time = passed.groupby([\"candidate_id\", \"date_start_job\"])[\"education_level\"].max()\n",
    "    \n",
    "    df_work.set_index([\"candidate_id\", \"date_start_job\"], inplace=True)\n",
    "\n",
    "    # Store education data in candidate_work_experience\n",
    "    df_work[\"education\"] = education_through_time\n",
    "\n",
    "    # np.nan education = no education\n",
    "    df_work[\"education\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Reset index for further data augmentation\n",
    "    df_work.reset_index(inplace=True)\n",
    "        \n",
    "    return df_work\n",
    "\n",
    "candidate_work_experience = find_educations(candidate_work_experience, candidate_education)\n",
    "\n",
    "candidate_work_experience.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd6efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add skills\n",
    "# candidate_work_experience = pd.merge(candidate_work_experience, \n",
    "#                                      skills_ct, \n",
    "#                                      left_on=\"candidate_id\", \n",
    "#                                      right_index=True, \n",
    "#                                      how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aaf392",
   "metadata": {},
   "source": [
    "# Filtering and reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a434d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_5 = candidate_work_experience[\"isco_code4\"].value_counts()\n",
    "more_than_5 = set(more_than_5[more_than_5 > 5].index)\n",
    "candidate_work_experience = candidate_work_experience[candidate_work_experience[\"isco_code4\"].isin(more_than_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_idx = {v : i for i, v in enumerate(sorted(candidate_work_experience[\"function_id\"].astype(float).unique()))}\n",
    "idx_to_id = {i : v for i, v in id_to_idx.items()}\n",
    "\n",
    "code_to_idx = {v : i for i, v in enumerate(sorted(candidate_work_experience[\"isco_code4\"].astype(float).unique()))}\n",
    "idx_to_code = {i : v for i, v in code_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09475ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_work_experience[\"function_id\"] = candidate_work_experience[\"function_id\"].replace(id_to_idx)\n",
    "candidate_work_experience[\"isco_code4\"] = candidate_work_experience[\"isco_code4\"].replace(code_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_work_experience.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time it took to go from one job to another (in order)\n",
    "candidate_work_experience[\"time_between\"] = candidate_work_experience.groupby(\n",
    "    \"candidate_id\")[\"date_start_job\"].progress_apply(lambda x: x - x.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27157b0c",
   "metadata": {},
   "source": [
    "# Add CV embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pd = pd.read_parquet(\"s3://s3-nl-prd-semrb-emr/embeddings/doc_embeddings/word2vec/word2vec_doc_embedding.parquet\")\n",
    "embedding_pd['cv_id'] = embedding_pd['cv_id'].astype('int')\n",
    "embedding_pd.rename(columns={\"doc_embedding\": \"tensor\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_path = os.path.join(\"/home/ec2-user/SageMaker/data-science-development/talent_recommender/daily_snapshots\",\n",
    "                        Config.query_candidate_cvs) \n",
    "\n",
    "candidate_cvs = get_from_redshift(sql_path, Config.redshift_creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_embeddings(embedding_pd, candidate_cvs):\n",
    "    embedding_pd = embedding_pd.set_index(\"cv_id\")\n",
    "    \n",
    "    candidate_cvs = candidate_cvs[[\"cv_id\", \"candidate_id\", \"date_start\"]]\n",
    "\n",
    "    embedding_per_candidate = pd.merge(embedding_pd, \n",
    "                                       candidate_cvs, \n",
    "                                       left_index=True, \n",
    "                                       right_on=\"cv_id\")[[\"candidate_id\", \"embedding\", \"date_start\"]]\n",
    "    \n",
    "    matches = (set(embedding_per_candidate[\"candidate_id\"]) & set(candidate_work_experience[\"candidate_id\"]))\n",
    "    \n",
    "    matched_cvs = embedding_per_candidate[embedding_per_candidate[\"candidate_id\"].isin(matches)]\n",
    "    matched_cvs[\"date_start\"] = matched_cvs[\"date_start\"].dt.date\n",
    "    \n",
    "    last_cv_per_day = matched_cvs.groupby([\"candidate_id\", \"date_start\"])[\"embedding\"].apply(lambda x: list(x)[-1])\n",
    "    \n",
    "    cv_embeddings = last_cv_per_day.reset_index()\n",
    "    \n",
    "    return cv_embeddings\n",
    "\n",
    "cv_embeddings = find_embeddings(embedding_pd, candidate_cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb60f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9250e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(candidate_work_experience, cv_embeddings):\n",
    "    \n",
    "    # Add CVs to candidates (includes duplicates)\n",
    "    full_merge = pd.merge(candidate_work_experience,\n",
    "                          cv_embeddings,\n",
    "                          left_on=\"candidate_id\",\n",
    "                          right_on=\"candidate_id\",\n",
    "                          how=\"left\")\n",
    "    \n",
    "    display(full_merge)\n",
    "    \n",
    "    # Find the maximum date of uploaded cvs per job (i.e. most recent CV during each job)\n",
    "    most_recent_cvs = full_merge[(full_merge[\"date_start_job\"]\n",
    "                                  >= full_merge[\"date_start\"])].groupby(\n",
    "        \"unique_id\")[\"date_start\"].idxmax()   \n",
    "    \n",
    "    # Some candidates only added CVs after their last job started, so account for that too\n",
    "    late_cvs = full_merge[(full_merge[\"date_start_job\"] <= \n",
    "                           full_merge[\"date_start\"])].groupby(\"unique_id\")[\"date_start\"].idxmax()\n",
    "    \n",
    "    # Filter out everything we already found earlier\n",
    "    late_cvs = late_cvs[set(late_cvs.index) - set(most_recent_cvs.index)]\n",
    "    \n",
    "    # Now we have all the CVs in one place\n",
    "    cv_idxs = pd.concat([most_recent_cvs, late_cvs])\n",
    "    \n",
    "    # Combine the two frames to include candidates without any CVs\n",
    "    combined = pd.concat([full_merge[full_merge[\"embedding\"].isna()], \n",
    "                          full_merge.loc[cv_idxs.values]])\n",
    "    \n",
    "    return combined\n",
    "\n",
    "candidate_work_experience = add_embeddings(candidate_work_experience, cv_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_work_experience[\"embedding\"] = candidate_work_experience[\"embedding\"].apply(lambda x: x if x is not np.nan\n",
    "#                                                                                                   else [0] * 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fdf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_work_experience = candidate_work_experience.reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398452fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_lists = candidate_work_experience[\"embedding\"].to_list()\n",
    "# embeddings = pd.DataFrame(embedding_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.columns = [f\"embedding_{i}\" for i in range(len(embeddings.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d42ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_work_experience = pd.concat([candidate_work_experience, embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8579bb7",
   "metadata": {},
   "source": [
    "# Ordering & Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dff867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_order(df):\n",
    "    \n",
    "    # Count the number of jobs each candidate has ahd\n",
    "    job_counts = df.groupby(\"candidate_id\").size()  \n",
    "    \n",
    "    # Sort by candidate_id, date_start_job\n",
    "    sorted_df = df.sort_values(by = ['candidate_id', \"date_start_job\"])\n",
    "    \n",
    "    # Reset index\n",
    "    sorted_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Create a list of lists containing the order of each candidates jobs (which came first, second, third, etc.)\n",
    "    order = [np.arange(count) for count in job_counts.values]\n",
    "    \n",
    "    # Flatten list\n",
    "    order = [item for sublist in order for item in sublist]\n",
    "    \n",
    "    # Add order to df\n",
    "    sorted_df[\"job_order\"] = order\n",
    "    \n",
    "    # Set a candidate_id, job_order as the index\n",
    "    return sorted_df.set_index([\"candidate_id\", \n",
    "                                \"job_order\"])\n",
    "\n",
    "df = add_order(candidate_work_experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d424f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[[\"date_start_job\", \"date_end_job\", \"time_spent\", \"time_between\", \"isco_code4\", \"function_id\",\n",
    "         \"isco_functie_niveau\", \"company_name\", \"source\", \"education\", \"embedding\"]]\n",
    "\n",
    "df[\"time_between\"] = df[\"time_between\"].shift(-1).fillna(pd.Timedelta(seconds=0)).dt.days.astype('int16')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df[\"isco_code4\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred = df[[\"isco_functie_niveau\", \"education\", \"function_id\"]].fillna(0)\n",
    "pred_cols = [\"time_between\", \"time_spent\", \"isco_functie_niveau\", \n",
    "             \"education\", \"function_id\", \"isco_code4\"] # + [col for col in df.columns if \"skill_\" in col]\n",
    "num_features = len(pred_cols)\n",
    "\n",
    "df_pred = df[pred_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d63e23",
   "metadata": {},
   "source": [
    "### Create separate embedding dict\n",
    "\n",
    "Done for optimization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc087a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_order = df.reset_index()[[\"candidate_id\", \"job_order\", \"embedding\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_embeddings = embedding_order.groupby(\"candidate_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea64f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_a = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Find each time step at which the candidate got a new CV\n",
    "for candidate, values in tqdm(grouped_embeddings):\n",
    "    embeddings = values[\"embedding\"].values\n",
    "  \n",
    "    if (type(embeddings[0]) == type(np.array([]))) and np.nan not in embeddings[0]:\n",
    "        embedding_a[candidate][0] = embeddings[0]\n",
    "        \n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            if i > 0:\n",
    "                truths = embeddings[i - 1] != embedding\n",
    "\n",
    "                if (type(truths) == type(np.array([]))) and (truths).all():\n",
    "                    embedding_a[candidate][i] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.json', 'w') as f:\n",
    "    json.dump(embedding_a, f, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9284b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.reset_index().to_csv(\"df_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(skills_dict).to_csv(\"skills.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf8e20",
   "metadata": {},
   "source": [
    "### Candidate Certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7eed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_path = os.path.join(\"/home/ec2-user/SageMaker/data-science-development/talent_recommender/daily_snapshots\",\n",
    "                        Config.query_candidate_certificates) \n",
    "\n",
    "candidate_certificates = get_from_redshift(sql_path, Config.redshift_creds)\n",
    "candidate_certificates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_certificates = pd.crosstab(candidate_certificates[\"candidate_id\"], \n",
    "                                     candidate_certificates[\"candidate_certificate_id\"])\n",
    "candidate_certificates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab209c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_certificates.to_csv(\"candidate_certificates_one-hot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95c7fc",
   "metadata": {},
   "source": [
    "### CV Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(\"df_pred.csv\")\n",
    "skills = pd.read_csv(\"skills.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea335d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = s3fs.S3FileSystem()\n",
    "\n",
    "# embedding_pd = pq.ParquetDataset('s3a://s3-nl-prd-semrb-emr/embeddings/doc_embeddings/xlm-roberta-base-smartmatch', filesystem=s3).read_pandas().to_pandas()\n",
    "\n",
    "# embedding_pd['cv_id'] = embedding_pd['cv_id'].astype('int')\n",
    "# embedding_pd.rename(columns={\"doc_embedding\": \"tensor\"}, inplace=True)\n",
    "# embedding_pd.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b807477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f9496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvPath = 's3a://s3-nl-prd-datahub-projects/smartmatch_cv/parsed_cv'\n",
    "\n",
    "# s3 = s3fs.S3FileSystem()\n",
    "# parsedCv_pd = pq.ParquetDataset(cvPath, filesystem=s3).read_pandas().to_pandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087fc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsedCv_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5cefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38290f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af341881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c5877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38468a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0753885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13036f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6308a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(cv_embeddings[\"candidate_id\"]) & set(df_pred[\"candidate_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_embeddings.to_csv(\"cv_w2v_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd46dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
