{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e542a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.10.1)\n",
      "Collecting torch\n",
      "  Using cached torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.1\n",
      "    Uninstalling torch-1.10.1:\n",
      "      Successfully uninstalled torch-1.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "torchvision 0.11.2 requires torch==1.10.1, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2deda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/utils\")\n",
    "path.append(\"/home/ec2-user/SageMaker/data-science-development/config\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74c1155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_1</th>\n",
       "      <th>skill_2</th>\n",
       "      <th>skill_3</th>\n",
       "      <th>skill_5</th>\n",
       "      <th>skill_6</th>\n",
       "      <th>skill_7</th>\n",
       "      <th>skill_8</th>\n",
       "      <th>skill_9</th>\n",
       "      <th>skill_12</th>\n",
       "      <th>skill_13</th>\n",
       "      <th>...</th>\n",
       "      <th>skill_3926</th>\n",
       "      <th>skill_3927</th>\n",
       "      <th>skill_3928</th>\n",
       "      <th>skill_3929</th>\n",
       "      <th>skill_3930</th>\n",
       "      <th>skill_3931</th>\n",
       "      <th>skill_3932</th>\n",
       "      <th>skill_3933</th>\n",
       "      <th>skill_3934</th>\n",
       "      <th>skill_3935</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84432</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              skill_1  skill_2  skill_3  skill_5  skill_6  skill_7  skill_8  \\\n",
       "candidate_id                                                                  \n",
       "84267               0        0        0        0        0        0        0   \n",
       "84349               1        0        0        0        0        0        0   \n",
       "84381               0        0        0        0        0        0        0   \n",
       "84386               0        0        0        0        0        0        0   \n",
       "84432               0        0        0        0        0        0        0   \n",
       "\n",
       "              skill_9  skill_12  skill_13  ...  skill_3926  skill_3927  \\\n",
       "candidate_id                               ...                           \n",
       "84267               0         0         0  ...           0           0   \n",
       "84349               0         0         0  ...           0           0   \n",
       "84381               0         0         0  ...           0           0   \n",
       "84386               0         0         0  ...           0           0   \n",
       "84432               0         0         0  ...           0           0   \n",
       "\n",
       "              skill_3928  skill_3929  skill_3930  skill_3931  skill_3932  \\\n",
       "candidate_id                                                               \n",
       "84267                  0           0           0           0           0   \n",
       "84349                  0           0           0           0           0   \n",
       "84381                  0           0           0           0           0   \n",
       "84386                  0           0           0           0           0   \n",
       "84432                  0           0           0           0           0   \n",
       "\n",
       "              skill_3933  skill_3934  skill_3935  \n",
       "candidate_id                                      \n",
       "84267                  0           0           0  \n",
       "84349                  0           0           0  \n",
       "84381                  0           0           0  \n",
       "84386                  0           0           0  \n",
       "84432                  0           0           0  \n",
       "\n",
       "[5 rows x 317 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_csv(\"../Data/skills_one-hot.csv\").set_index(\"candidate_id\")\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb8f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = dict(zip(skills.index, skills.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2468f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>W4</th>\n",
       "      <th>W5</th>\n",
       "      <th>W7</th>\n",
       "      <th>W9</th>\n",
       "      <th>WB</th>\n",
       "      <th>WC</th>\n",
       "      <th>WD</th>\n",
       "      <th>WE</th>\n",
       "      <th>WF</th>\n",
       "      <th>ZW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84603</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1  10  11  12  13  14  15  16  17  18  ...  W4  W5  W7  W9  WB  \\\n",
       "candidate_id                                         ...                       \n",
       "84603         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "84867         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "85035         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "85102         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "85214         0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "\n",
       "              WC  WD  WE  WF  ZW  \n",
       "candidate_id                      \n",
       "84603          0   0   0   0   0  \n",
       "84867          0   0   0   0   0  \n",
       "85035          0   0   0   0   0  \n",
       "85102          0   0   0   0   0  \n",
       "85214          0   0   0   0   0  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certs = pd.read_csv(\"../Data/candidate_certificates_one-hot.csv\").set_index(\"candidate_id\")\n",
    "certs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3424cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "certs = dict(zip(certs.index, certs.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e497cdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>BE</th>\n",
       "      <th>C</th>\n",
       "      <th>CE</th>\n",
       "      <th>D</th>\n",
       "      <th>DE</th>\n",
       "      <th>G</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84556</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84731</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85437</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85627</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A  B  BE  C  CE  D  DE  G\n",
       "candidate_id                           \n",
       "84556         0  1   0  0   0  0   0  0\n",
       "84612         0  0   0  0   0  0   0  1\n",
       "84731         1  1   0  0   0  0   0  0\n",
       "85437         0  1   0  0   0  0   0  0\n",
       "85627         0  1   1  0   0  0   0  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licenses = pd.read_csv(\"../Data/licenses_one-hot.csv\").set_index(\"candidate_id\")\n",
    "licenses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59792fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses = dict(zip(licenses.index, licenses.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6bfc9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84349</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84432</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0  1  2  3  4  5  6  7  8  9  ...  13  14  15  16  17  18  19  \\\n",
       "candidate_id                                ...                               \n",
       "84267         0  0  1  1  1  0  0  0  0  0  ...   0   0   0   0   0   0   0   \n",
       "84349         0  0  1  1  0  0  1  0  0  0  ...   0   0   0   0   0   0   0   \n",
       "84381         0  0  0  1  0  0  0  0  0  1  ...   0   0   0   0   0   0   0   \n",
       "84386         0  0  1  1  0  0  1  0  0  0  ...   0   0   0   0   0   1   0   \n",
       "84432         0  0  0  1  0  0  1  0  0  0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "              20  21  22  \n",
       "candidate_id              \n",
       "84267          0   0   0  \n",
       "84349          0   0   0  \n",
       "84381          0   0   0  \n",
       "84386          0   0   0  \n",
       "84432          0   0   0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = pd.read_csv(\"../Data/languages_one-hot.csv\").set_index(\"candidate_id\")\n",
    "languages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e629517",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = dict(zip(languages.index, languages.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b44e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84556</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84612</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84731</th>\n",
       "      <td>3773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85437</th>\n",
       "      <td>3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85627</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "candidate_id      \n",
       "84556           91\n",
       "84612           49\n",
       "84731         3773\n",
       "85437         3819\n",
       "85627         1560"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses = pd.read_csv(\"../Data/addresses_one-hot.csv\").set_index(\"candidate_id\")\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26766cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = dict(zip(addresses.index, addresses.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc39b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = json.load(open(\"../Data/embeddings.json\"))\n",
    "# Convert to ints\n",
    "w2v = {int(k):{int(k2):v2 for k2, v2 in v.items()} for k, v in w2v.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1413d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(\"../Data/df_pred_ext.csv\").drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80304dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred.drop([\"time_between\", \"job_order\", \"source\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "482f3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred[\"time_between\"] = (df_pred[\"time_between\"] - df_pred[\"time_between\"].mean()) / df_pred[\"time_between\"].std()\n",
    "df_pred[\"time_spent\"] = (df_pred[\"time_spent\"] - df_pred[\"time_spent\"].mean()) / df_pred[\"time_spent\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5c08c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>isco_functie_niveau</th>\n",
       "      <th>education</th>\n",
       "      <th>company_name</th>\n",
       "      <th>function_id</th>\n",
       "      <th>isco_code4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84556</td>\n",
       "      <td>-0.210459</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324258</td>\n",
       "      <td>936</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84556</td>\n",
       "      <td>-0.252626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324258</td>\n",
       "      <td>809</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84556</td>\n",
       "      <td>-0.085012</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>329244</td>\n",
       "      <td>936</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84556</td>\n",
       "      <td>-0.370694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>368140</td>\n",
       "      <td>1519</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84556</td>\n",
       "      <td>-0.363314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419826</td>\n",
       "      <td>1519</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   candidate_id  time_spent  isco_functie_niveau  education  company_name  \\\n",
       "0         84556   -0.210459                  2.0        0.0        324258   \n",
       "1         84556   -0.252626                  1.0        0.0        324258   \n",
       "2         84556   -0.085012                  2.0        0.0        329244   \n",
       "3         84556   -0.370694                  1.0        0.0        368140   \n",
       "4         84556   -0.363314                  1.0        0.0        419826   \n",
       "\n",
       "   function_id  isco_code4  \n",
       "0          936         208  \n",
       "1          809         348  \n",
       "2          936         208  \n",
       "3         1519         344  \n",
       "4         1519         344  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c529ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "career_paths = df_pred.groupby(\"candidate_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc2f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lens = career_paths.apply(lambda x: len(x) - 1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da20cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(df_pred[\"isco_code4\"].unique())\n",
    "num_features = len(career_paths.mean().columns)\n",
    "num_classes, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e455dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_career_duration = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854f0719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469568/469568 [00:49<00:00, 9435.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to 2d-arrays, grabbing the last 25 jobs of each candidate and getting rid of candidate_ids as values\n",
    "career_paths = career_paths.progress_apply(lambda x: x.values[-(maximum_career_duration + 1):,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fbe68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop careers that are only 1 job long\n",
    "career_lens = career_paths.apply(len)\n",
    "career_paths = career_paths.loc[(career_lens > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e96312d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "career_paths = career_paths.loc[career_paths.apply(lambda x: x[-1][-1] != x[-2][-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f0173a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidate_id\n",
       "84556    [[-0.21045870102048395, 2.0, 0.0, 324258.0, 93...\n",
       "84612    [[-0.3685852264755267, 1.0, 0.0, 201740.0, 151...\n",
       "84731    [[-0.35066422025728855, 1.0, 0.0, 353745.0, 15...\n",
       "85437    [[0.3313881928721292, 1.0, 2.0, 5500.0, 1519.0...\n",
       "85888    [[-0.2895219637480053, 2.0, 3.0, 423330.0, 795...\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "career_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e311f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# max_skills = len([col for col in df_pred if \"skill_\" in col])\n",
    "\n",
    "for idx, career in zip(career_paths.index, career_paths.values):\n",
    "    label = career[-1, -1]\n",
    "    \n",
    "    if not np.isnan(label):       \n",
    "        idxs.append(idx)\n",
    "        x.append(career[:-1].reshape(len(career) - 1, num_features))\n",
    "        y.append(label)\n",
    "\n",
    "idxs = np.array(idxs)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1545fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill = np.zeros([len(x), len(max(x, key = lambda x: len(x))), num_features])\n",
    "\n",
    "for i,j in enumerate(x):\n",
    "    if len(j):\n",
    "        to_fill[i][-len(j):] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5e126cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = len(max(x, key = lambda x: len(x)))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbdb4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pred\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7900ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6056cb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113724, 113724)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_fill), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b991081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_fill = to_fill[:75000]\n",
    "# y = y[:75000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "651e5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(to_fill, idxs, y, split_size=0.8, weight_type=3, batch_size=512):\n",
    "\n",
    "    # Train test split\n",
    "    split = split_size\n",
    "\n",
    "    training = np.array(random.sample(range(len(to_fill)), int(split * len(to_fill))))\n",
    "    test = np.array(list(set(range(len(to_fill))) - set(training)))\n",
    "    test, validation = test[:(len(test) // 2)], test[(len(test) // 2):]\n",
    "\n",
    "    train_indices, val_indices, test_indices = idxs[training], idxs[validation], idxs[test]\n",
    "    X_train, X_val, X_test = to_fill[training], to_fill[validation], to_fill[test]\n",
    "    y_train, y_val, y_test = y[training].astype(int), y[validation].astype(int), y[test].astype(int)\n",
    "\n",
    "    # Class weights\n",
    "    counts = (np.bincount(y_train) + 1)\n",
    "    \n",
    "    if weight_type == 1:\n",
    "        labels_weights = 1. / counts\n",
    "    elif weight_type == 2:\n",
    "        labels_weights = 1. / np.sqrt(counts)\n",
    "    elif weight_type == 3:\n",
    "        labels_weights = 2. / (0.5 * np.sqrt(counts))\n",
    "    else:\n",
    "        return NotImplemented\n",
    "        \n",
    "    weights = labels_weights[y_train]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_data = TensorDataset(torch.Tensor(train_indices), \n",
    "                               torch.Tensor(X_train), \n",
    "                               torch.Tensor(y_train).type(torch.LongTensor))\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    val_data = TensorDataset(torch.Tensor(val_indices),\n",
    "                             torch.Tensor(X_val),\n",
    "                             torch.Tensor(y_val).type(torch.LongTensor))\n",
    "\n",
    "    valloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_data = TensorDataset(torch.Tensor(test_indices),\n",
    "                             torch.Tensor(X_test),\n",
    "                             torch.Tensor(y_test).type(torch.LongTensor))\n",
    "\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6be3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, \n",
    "                 kernel_size, kernel_size2, F1, F2, dropout, \n",
    "                 num_layers, skills, certs, licenses, languages, \n",
    "                 addresses, w2v, candidate_lengths, max_len, \n",
    "                 skill_embedding_size=50, certs_embedding_size=20,\n",
    "                 license_embedding_size=3, language_embedding_size=10,\n",
    "                 address_embedding_size=25, function_embedding_size=50, \n",
    "                 isco4_embedding_size=25, education_embedding_size=3, \n",
    "                 isco_level_embedding_size=3, company_embedding_size=50):\n",
    "        \n",
    "        super(CNN_LSTM, self).__init__()\n",
    "              \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size + 300\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Static embeddings: skills, certificates, licenses, languages\n",
    "        self.skill_embedding = nn.Linear(317, skill_embedding_size, bias=False)\n",
    "        self.skill_embedding.weight.data = torch.randn_like(self.skill_embedding.weight) \n",
    "        \n",
    "        self.certs_embedding = nn.Linear(98, certs_embedding_size, bias=False)\n",
    "        self.certs_embedding.weight.data = torch.randn_like(self.certs_embedding.weight) \n",
    "        \n",
    "        self.license_embedding = nn.Linear(8, license_embedding_size, bias=False)\n",
    "        self.license_embedding.weight.data = torch.randn_like(self.license_embedding.weight) \n",
    "        \n",
    "        self.language_embedding = nn.Linear(23, language_embedding_size, bias=False)\n",
    "        self.language_embedding.weight.data = torch.randn_like(self.language_embedding.weight) \n",
    "        \n",
    "        # Address embedding\n",
    "        self.address_embedding = nn.Embedding(4768, address_embedding_size)       \n",
    "        \n",
    "        # Categorical feature embeddings isco_functie_niveau\tsource\teducation\tcompany_name\tfunction_id\tisco_code4\n",
    "        self.function_embedding = nn.Embedding(2992, function_embedding_size)\n",
    "        self.isco_code_embedding = nn.Embedding(num_classes, isco4_embedding_size)\n",
    "        self.company_embedding = nn.Embedding(441153, company_embedding_size)\n",
    "        self.education_embedding = nn.Embedding(6, education_embedding_size)\n",
    "        self.isco_level_embedding = nn.Embedding(5, isco_level_embedding_size)\n",
    "                \n",
    "        # -5 --> embedded features get replaced\n",
    "        N = self.input_size - 5 + skill_embedding_size + certs_embedding_size + \\\n",
    "            license_embedding_size + language_embedding_size + address_embedding_size + \\\n",
    "            function_embedding_size + isco4_embedding_size + company_embedding_size + \\\n",
    "            education_embedding_size + isco_level_embedding_size\n",
    "                \n",
    "        self.conv_padding = nn.ZeroPad2d((0, 0,\n",
    "                                          kernel_size // 2, (kernel_size - 1) // 2))\n",
    "        \n",
    "        self.conv32 = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=F1,\n",
    "                                kernel_size=(kernel_size, 1), \n",
    "                                stride=1)\n",
    "        \n",
    "        self.conv64 = nn.Conv2d(in_channels=F1, \n",
    "                                out_channels=F2,\n",
    "                                kernel_size=(kernel_size, 1), \n",
    "                                stride=1)\n",
    "        \n",
    "        self.avgpooling = nn.AvgPool3d(kernel_size=(F2, 1, kernel_size2))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "            \n",
    "        self.lstm = nn.LSTM(input_size=N // kernel_size2,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "            \n",
    "        # Final fully-connected layer takes the LSTM output, as well as the static embeddings\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Skill lookup\n",
    "        self.skills = skills\n",
    "        \n",
    "        # Certificate lookup\n",
    "        self.certs = certs\n",
    "        \n",
    "        # License lookup\n",
    "        self.licenses = licenses\n",
    "        \n",
    "        # Language lookup\n",
    "        self.langs = languages\n",
    "        \n",
    "        # Address lookup\n",
    "        self.adds = addresses\n",
    "        \n",
    "        # w2v lookup\n",
    "        self.w2v_keys = set(w2v.keys())\n",
    "        self.w2v = w2v\n",
    "        \n",
    "        # Career durations\n",
    "        self.candidate_lengths = candidate_lengths\n",
    "        self.max_len = max_len      \n",
    "        \n",
    "        def get_from_dict(x, cdict, N):\n",
    "            return cdict.get(x, np.zeros((N,)))\n",
    "\n",
    "        self.retrieve_static = np.vectorize(get_from_dict, otypes=[np.ndarray])  \n",
    "                \n",
    "    def w2v_lookup(self, candidate, career_duration):\n",
    "        \"\"\"Finds a candidate's CVs and converts them to a tensor of length career_duration\"\"\"\n",
    "            \n",
    "        actual_career_duration = career_duration\n",
    "        career_duration = min(career_duration, max_len)\n",
    "            \n",
    "        # Look for cvs\n",
    "        if candidate.item() in self.w2v_keys:\n",
    "            cvs = self.w2v[candidate.item()]\n",
    "                \n",
    "            storage = []\n",
    "\n",
    "             # If a candidate only has one CV, proceed as normal\n",
    "            if len(cvs.keys()) == 1:\n",
    "                w2v_list = torch.LongTensor(cvs[0]).to(device)\n",
    "                w2v_list = torch.stack([w2v_list] * career_duration)\n",
    "            else: # Otherwise, stack them accordingly\n",
    "                ks = np.array(list(cvs.keys()))\n",
    "                \n",
    "                to_skip = 0\n",
    "                                \n",
    "                # Make sure to use candidates' most recent max_len cvs\n",
    "                if actual_career_duration > self.max_len:\n",
    "                    # 0, 10, 20, 30, 40, 50\n",
    "                    # duration = 50\n",
    "                    # ---> 0, 5, 15, 25\n",
    "                                        \n",
    "                    # Update to only include most recent max_len\n",
    "                    ks -= max_len\n",
    "                    \n",
    "                    # Drop everything older than max_len time steps\n",
    "                    ks_2 = np.array([ks[i] for i in range(len(ks)) if i < len(ks) and (i + 1 >= len(ks) or ks[i + 1] > 0)])\n",
    "                    \n",
    "                    # Store how many we need to skip while indexing\n",
    "                    to_skip = len(ks) - len(ks_2)\n",
    "                    \n",
    "                    # Update ks\n",
    "                    ks = ks_2\n",
    "                    ks[0] = 0\n",
    "                    \n",
    "                # Due to clipping, some careers are longer than max_len\n",
    "                ks = np.array([k for k in ks if k <= min(self.max_len, career_duration)])\n",
    "\n",
    "                # Find how many time steps (rows) each CV lasted\n",
    "                durations = [ks[i+1] - ks[i]\n",
    "                             if i < (len(ks) - 1) \n",
    "                             else career_duration - ks[i]\n",
    "                             for i in range(len(ks))]\n",
    "\n",
    "                embed_values = list(cvs.values())\n",
    "\n",
    "                # When the CV got updated on the last timestep, aka our test value\n",
    "                # Remove it from the list of durations, as it should be ignored\n",
    "                if durations[-1] == 0: \n",
    "                    durations.pop()\n",
    "\n",
    "                # Create Tensor(s)\n",
    "                if durations:\n",
    "                    for i, duration in enumerate(durations):\n",
    "                        # Figure out negative duration cause\n",
    "                        storage.append(torch.stack([torch.Tensor(embed_values[i + to_skip])] * duration, dim=0))\n",
    "                else:\n",
    "                    w2v_list = torch.LongTensor(cvs[0]).to(device)\n",
    "\n",
    "                # Combine stored tensors into a single tensor\n",
    "                w2v_list = torch.cat((storage)).type(torch.LongTensor).to(device)\n",
    "        else:\n",
    "            w2v_list = torch.LongTensor([0] * 300).to(device)\n",
    "            w2v_list = torch.stack([w2v_list] * career_duration)\n",
    "\n",
    "        return w2v_list\n",
    " \n",
    "    def forward(self, candidate, x):               \n",
    "        # Default width of a row (filled with 0s)\n",
    "        feature_width = torch.Tensor([0] * 500).type(torch.LongTensor).to(device)\n",
    "        \n",
    "        candidate_features = []\n",
    "        \n",
    "        skill_list = self.retrieve_static(candidate, self.skills, 317)\n",
    "        skill_list = torch.LongTensor(np.stack(skill_list)).to(device)\n",
    "        \n",
    "        certs_list = self.retrieve_static(candidate, self.certs, 98)\n",
    "        certs_list = torch.LongTensor(np.stack(certs_list)).to(device)\n",
    "        \n",
    "        license_list = self.retrieve_static(candidate, self.licenses, 8)\n",
    "        license_list = torch.LongTensor(np.stack(license_list)).to(device)\n",
    "        \n",
    "        langs_list = self.retrieve_static(candidate, self.langs, 23)\n",
    "        langs_list = torch.LongTensor(np.stack(langs_list)).to(device)\n",
    "            \n",
    "        address = self.retrieve_static(candidate, self.adds, 1)\n",
    "        address = torch.LongTensor(np.stack(address)).to(device)\n",
    "        \n",
    "        # Embed every static feature\n",
    "        skill_list, certs_list, license_list, langs_list = [self.skill_embedding(skill_list.type(torch.FloatTensor).to(device)),\n",
    "                                                            self.certs_embedding(certs_list.type(torch.FloatTensor).to(device)),\n",
    "                                                            self.license_embedding(license_list.type(torch.FloatTensor).to(device)),\n",
    "                                                            self.language_embedding(langs_list.type(torch.FloatTensor).to(device))]\n",
    "        \n",
    "        # Combine and embed\n",
    "        batch_features = torch.cat([skill_list, certs_list, \n",
    "                                    license_list, langs_list], dim=-1).type(torch.FloatTensor).to(device)\n",
    "            \n",
    "        batch_addresses = self.address_embedding(address)[:,0,:]\n",
    "                \n",
    "        # For each candidate in the current batch\n",
    "        for i, c in enumerate(candidate):\n",
    "            # Get career duration\n",
    "            career_duration = self.candidate_lengths[c.item()]\n",
    "                        \n",
    "            # Get CV embeddings\n",
    "            w2v_list = self.w2v_lookup(c, career_duration)\n",
    "            \n",
    "            # Reset to max_len\n",
    "            career_duration = min(career_duration, max_len)\n",
    "\n",
    "            # Only create zeros if needed (e.g. less than max_len career duration)\n",
    "            if (self.max_len - career_duration) > 0:\n",
    "                zeros = torch.stack([feature_width] * (self.max_len - career_duration))\n",
    "            else: # Reset zeros to prevent shape mismatch\n",
    "                zeros = torch.LongTensor([]).to(device)\n",
    "                   \n",
    "            # Broadcast and add static features\n",
    "            static_features = torch.stack([batch_features[i]] * career_duration).type(torch.LongTensor).to(device)\n",
    "            address_emb = torch.stack([batch_addresses[i]] * career_duration).type(torch.LongTensor).to(device)\n",
    "            \n",
    "            # Combine w2v, static features, and address\n",
    "            full_features = torch.cat([w2v_list, static_features, address_emb], dim=1)\n",
    "                                    \n",
    "            # Broadcast CV, static, and address to the correct length\n",
    "            full_features = torch.cat([zeros, full_features], dim=0)\n",
    "                    \n",
    "            # Store result\n",
    "            candidate_features.append(full_features)\n",
    "                                \n",
    "        # Convert list of tensors to actual tensor\n",
    "        additional_features = torch.stack((candidate_features)).type(torch.FloatTensor).to(device)\n",
    "                \n",
    "        # isco_functie_niveau, education, function_id, isco_code4\n",
    "        isco_level, education, company_name, function_id, isco_code = [x[:,:,-5],\n",
    "                                                                       x[:,:,-4],\n",
    "                                                                       x[:,:,-3],\n",
    "                                                                       x[:,:,-2],\n",
    "                                                                       x[:,:,-1]]\n",
    "        \n",
    "        x = x[:,:,:-5].to(device)\n",
    "        \n",
    "        isco_level_smoothing = (isco_level != 0).unsqueeze(-1)\n",
    "        education_smoothing = (education != 0).unsqueeze(-1)\n",
    "        company_name_smoothing = (company_name != 0).unsqueeze(-1)\n",
    "        function_id_smoothing = (function_id != 0).unsqueeze(-1)\n",
    "        isco_code_smoothing = (isco_code != 0).unsqueeze(-1)\n",
    "                \n",
    "        isco_level, education, company_name, function_id, isco_code  = [self.isco_level_embedding(isco_level.type(torch.LongTensor).to(device)) * isco_level_smoothing,\n",
    "                                                                        self.education_embedding(education.type(torch.LongTensor).to(device)) * education_smoothing,\n",
    "                                                                        self.company_embedding(company_name.type(torch.LongTensor).to(device)) * company_name_smoothing,\n",
    "                                                                        self.function_embedding(function_id.type(torch.LongTensor).to(device)) * function_id_smoothing,\n",
    "                                                                        self.isco_code_embedding(isco_code.type(torch.LongTensor).to(device)) * isco_code_smoothing]   \n",
    "                \n",
    "        # Add features\n",
    "        x = torch.cat([x, isco_level, education, company_name, function_id, isco_code, additional_features], dim=2)\n",
    "        x = x.unsqueeze(1)\n",
    "                                       \n",
    "        # Forward pass\n",
    "        x = self.conv_padding(x)\n",
    "        x = self.conv32(x)\n",
    "        x = self.conv_padding(x)\n",
    "        x = self.conv64(x)\n",
    "                \n",
    "        # Apply maxpooling\n",
    "        x = self.avgpooling(x)\n",
    "                                            \n",
    "        # Get rid of extra dimension\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "            \n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        \n",
    "        x = h_n.squeeze(0)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully-connected\n",
    "        out = self.fc(x)\n",
    "    \n",
    "        # softmax\n",
    "        out = self.softmax(out)\n",
    "                        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b962ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, trainloader, valloader, testloader, optimizer, scheduler, criterion, num_epochs):\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    passed = [0]\n",
    "    training_losses = [6]\n",
    "    test_losses = [6]\n",
    "    accuracy = [0]\n",
    "    \n",
    "    highest_performance = 0\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"Epoch starting at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        training_loss = 0\n",
    "\n",
    "        for i, (candidate, career, job) in enumerate(trainloader):\n",
    "            \n",
    "            career, job = career.to(device), job.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(candidate, career)\n",
    "                        \n",
    "            # obtain the loss function\n",
    "            loss = criterion(outputs, job)\n",
    "            loss = loss.mean()           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "            print(\"Epoch: %d, batch: %d/%d, loss: %1.5f\" % (epoch + 1, i + 1, len(trainloader), loss.item()), end=\"\\r\")\n",
    "               \n",
    "        training_loss /= len(trainloader)\n",
    "                \n",
    "        done = int(time.time() - start)        \n",
    "        print(f\"Epoch duration: {int(done // 60)}:{int(done % 60):02d}\")\n",
    "            \n",
    "        stats = test_loop(valloader, testloader, model, criterion)\n",
    "        results[\"Epoch\"].append(epoch + 1)\n",
    "        results[\"Acc@1\"].append(stats[0])\n",
    "        results[\"Acc@5\"].append(stats[1])\n",
    "        results[\"Acc@10\"].append(stats[2])\n",
    "        results[\"test_loss\"].append(stats[3])\n",
    "        results[\"Acc@1 (test)\"].append(stats[4])\n",
    "        results[\"Acc@5 (test)\"].append(stats[5])\n",
    "        results[\"Acc@10 (test)\"].append(stats[6])\n",
    "        results[\"test_loss (test)\"].append(stats[7])\n",
    "        results[\"training_loss\"].append(training_loss)\n",
    "        results[\"duration\"].append(done)\n",
    "        \n",
    "        if stats[0] > highest_performance:\n",
    "            torch.save(model.state_dict(), \"../models/CNN-LSTM_final.pt\")\n",
    "            highest_performance = stats[0]\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        passed.append(epoch + 1)\n",
    "        training_losses.append(training_loss)\n",
    "        test_losses.append(stats[4])\n",
    "        accuracy.append(stats[0])\n",
    "        \n",
    "#         plt.plot(passed, training_losses, label=\"Training Loss\")\n",
    "#         plt.plot(passed, test_losses, label=\"Test Loss\")\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"Average loss\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "                \n",
    "    return results\n",
    "        \n",
    "def test_loop(dataloader, testloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, acc1, acc5, acc10 = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for candidate, career, job in dataloader:\n",
    "            career, job = career.to(device), job.to(device)\n",
    "            pred = model(candidate, career)\n",
    "            \n",
    "            test_loss += criterion(pred, job).mean().item()\n",
    "            acc1 += (pred.argmax(1) == job).type(torch.float).sum().item()\n",
    "            \n",
    "            sorted_preds = torch.argsort(pred, 1, descending=True)\n",
    "            \n",
    "            at5 = []\n",
    "            at10 = []\n",
    "            \n",
    "            for answer, predictions in zip(job, sorted_preds):\n",
    "                at5.append(answer.item() in predictions[:5])\n",
    "                at10.append(answer.item() in predictions[:10])\n",
    "            \n",
    "            acc5 += np.sum(at5)\n",
    "            acc10 += np.sum(at10)\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    acc1 /= size\n",
    "    acc5 /= size\n",
    "    acc10 /= size\n",
    "    print(f\"\\nVal Error:\")\n",
    "    print(f\"Acc@1: {(100*acc1):>0.2f}%, Acc@5: {100*acc5:>0.2f}%, \" +\\\n",
    "          f\"Acc@10: {100*acc10:>0.2f}% Avg loss: {test_loss:>8f}\")\n",
    "    \n",
    "    size_2 = len(testloader.dataset)\n",
    "    num_batches_2 = len(testloader)\n",
    "    test_loss_2, acc1_2, acc5_2, acc10_2 = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for candidate, career, job in testloader:\n",
    "            career, job = career.to(device), job.to(device)\n",
    "            pred = model(candidate, career)\n",
    "            \n",
    "            test_loss_2 += criterion(pred, job).mean().item()\n",
    "            acc1_2 += (pred.argmax(1) == job).type(torch.float).sum().item()\n",
    "            \n",
    "            sorted_preds = torch.argsort(pred, 1, descending=True)\n",
    "            \n",
    "            at5_2 = []\n",
    "            at10_2 = []\n",
    "            \n",
    "            for answer, predictions in zip(job, sorted_preds):\n",
    "                at5_2.append(answer.item() in predictions[:5])\n",
    "                at10_2.append(answer.item() in predictions[:10])\n",
    "            \n",
    "            acc5_2 += np.sum(at5_2)\n",
    "            acc10_2 += np.sum(at10_2)\n",
    "            \n",
    "    test_loss_2 /= num_batches_2\n",
    "    acc1_2 /= size_2\n",
    "    acc5_2 /= size_2\n",
    "    acc10_2 /= size_2\n",
    "    print(f\"\\nTest Error:\")\n",
    "    print(f\"Acc@1: {(100*acc1_2):>0.2f}%, Acc@5: {100*acc5_2:>0.2f}%, \" +\\\n",
    "          f\"Acc@10: {100*acc10_2:>0.2f}%, Avg loss: {test_loss_2:>8f}\")\n",
    "    \n",
    "    return acc1, acc5, acc10, test_loss, acc1_2, acc5_2, acc10_2, test_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a727978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Initial learning rate: 0.001\n",
      "- Model: \n",
      "\n",
      " CNN_LSTM(\n",
      "  (skill_embedding): Linear(in_features=317, out_features=100, bias=False)\n",
      "  (certs_embedding): Linear(in_features=98, out_features=50, bias=False)\n",
      "  (license_embedding): Linear(in_features=8, out_features=10, bias=False)\n",
      "  (language_embedding): Linear(in_features=23, out_features=15, bias=False)\n",
      "  (address_embedding): Embedding(4768, 25)\n",
      "  (function_embedding): Embedding(2992, 250)\n",
      "  (isco_code_embedding): Embedding(355, 150)\n",
      "  (company_embedding): Embedding(441153, 300)\n",
      "  (education_embedding): Embedding(6, 10)\n",
      "  (isco_level_embedding): Embedding(5, 10)\n",
      "  (conv_padding): ZeroPad2d(padding=(0, 0, 0, 0), value=0.0)\n",
      "  (conv32): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv64): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (avgpooling): AvgPool3d(kernel_size=(64, 1, 1), stride=(64, 1, 1), padding=0)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (lstm): LSTM(1221, 1000, batch_first=True)\n",
      "  (fc): Linear(in_features=1000, out_features=355, bias=True)\n",
      "  (softmax): LogSoftmax(dim=-1)\n",
      ") \n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:09:17\n",
      "Epoch duration: 5:48/711, loss: 2.95897\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 18.83%, Acc@5: 45.56%, Acc@10: 59.28% Avg loss: 3.657242\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 22.85%, Acc@5: 51.37%, Acc@10: 63.81%, Avg loss: 3.480617\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:15:45\n",
      "Epoch duration: 5:49/711, loss: 1.60067\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 20.62%, Acc@5: 47.37%, Acc@10: 61.02% Avg loss: 3.607790\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 22.80%, Acc@5: 51.69%, Acc@10: 65.28%, Avg loss: 3.462938\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:22:20\n",
      "Epoch duration: 5:49/711, loss: 1.38326\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 21.17%, Acc@5: 48.12%, Acc@10: 62.22% Avg loss: 3.691204\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 22.41%, Acc@5: 53.00%, Acc@10: 66.08%, Avg loss: 3.531175\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:28:55\n",
      "Epoch duration: 5:48/711, loss: 0.75240\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 21.36%, Acc@5: 49.66%, Acc@10: 63.29% Avg loss: 3.775222\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 23.77%, Acc@5: 54.50%, Acc@10: 67.02%, Avg loss: 3.622058\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:35:28\n",
      "Epoch duration: 5:48/711, loss: 0.78338\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 20.72%, Acc@5: 48.89%, Acc@10: 62.17% Avg loss: 3.916626\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 23.48%, Acc@5: 53.33%, Acc@10: 66.17%, Avg loss: 3.762308\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:41:55\n",
      "Epoch duration: 5:49/711, loss: 0.45300\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 21.60%, Acc@5: 50.40%, Acc@10: 63.92% Avg loss: 3.877276\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 24.57%, Acc@5: 55.55%, Acc@10: 67.94%, Avg loss: 3.720387\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:48:29\n",
      "Epoch duration: 5:48/711, loss: 0.34446\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 21.70%, Acc@5: 50.73%, Acc@10: 64.42% Avg loss: 3.904756\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 24.99%, Acc@5: 56.08%, Acc@10: 68.18%, Avg loss: 3.745627\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 13:55:03\n",
      "Epoch duration: 5:48/711, loss: 0.29518\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.20%, Acc@5: 51.33%, Acc@10: 64.87% Avg loss: 3.929307\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 25.47%, Acc@5: 56.48%, Acc@10: 68.20%, Avg loss: 3.777029\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:01:36\n",
      "Epoch duration: 5:48/711, loss: 0.32667\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.52%, Acc@5: 51.44%, Acc@10: 64.73% Avg loss: 3.963844\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 25.69%, Acc@5: 56.57%, Acc@10: 68.33%, Avg loss: 3.815562\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:08:10\n",
      "Epoch duration: 5:481/711, loss: 0.27052\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.61%, Acc@5: 51.69%, Acc@10: 64.98% Avg loss: 3.994619\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.46%, Acc@5: 56.51%, Acc@10: 68.34%, Avg loss: 3.849970\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:14:44\n",
      "Epoch duration: 5:491/711, loss: 0.27936\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.52%, Acc@5: 51.74%, Acc@10: 65.00% Avg loss: 3.997686\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.35%, Acc@5: 56.47%, Acc@10: 68.40%, Avg loss: 3.853151\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:21:11\n",
      "Epoch duration: 5:501/711, loss: 0.26830\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.58%, Acc@5: 51.63%, Acc@10: 65.09% Avg loss: 4.001789\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.33%, Acc@5: 56.55%, Acc@10: 68.47%, Avg loss: 3.856535\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:27:40\n",
      "Epoch duration: 5:501/711, loss: 0.28858\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.67%, Acc@5: 51.75%, Acc@10: 65.00% Avg loss: 4.008154\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.44%, Acc@5: 56.47%, Acc@10: 68.43%, Avg loss: 3.863145\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:34:16\n",
      "Epoch duration: 5:481/711, loss: 0.35495\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.70%, Acc@5: 51.80%, Acc@10: 64.93% Avg loss: 4.011797\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.42%, Acc@5: 56.45%, Acc@10: 68.43%, Avg loss: 3.867450\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:40:50\n",
      "Epoch duration: 5:481/711, loss: 0.22722\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.67%, Acc@5: 51.73%, Acc@10: 64.96% Avg loss: 4.017975\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.32%, Acc@5: 56.45%, Acc@10: 68.58%, Avg loss: 3.873375\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:47:17\n",
      "Epoch duration: 5:491/711, loss: 0.41174\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.69%, Acc@5: 51.75%, Acc@10: 64.95% Avg loss: 4.017967\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.40%, Acc@5: 56.46%, Acc@10: 68.57%, Avg loss: 3.874083\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 14:53:47\n",
      "Epoch duration: 5:491/711, loss: 0.28975\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.74%, Acc@5: 51.79%, Acc@10: 64.95% Avg loss: 4.019338\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.37%, Acc@5: 56.52%, Acc@10: 68.55%, Avg loss: 3.875101\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 15:00:22\n",
      "Epoch duration: 5:491/711, loss: 0.17984\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.73%, Acc@5: 51.80%, Acc@10: 64.95% Avg loss: 4.019776\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.37%, Acc@5: 56.52%, Acc@10: 68.62%, Avg loss: 3.874875\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 15:06:50\n",
      "Epoch duration: 5:491/711, loss: 0.22657\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.76%, Acc@5: 51.77%, Acc@10: 64.93% Avg loss: 4.020553\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.43%, Acc@5: 56.53%, Acc@10: 68.60%, Avg loss: 3.876072\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch starting at: 15:13:25\n",
      "Epoch duration: 5:491/711, loss: 0.24593\n",
      "\n",
      "Val Error:\n",
      "Acc@1: 22.76%, Acc@5: 51.73%, Acc@10: 64.94% Avg loss: 4.020477\n",
      "\n",
      "Test Error:\n",
      "Acc@1: 26.44%, Acc@5: 56.47%, Acc@10: 68.58%, Avg loss: 3.876754\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "current = 0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "full_results = []\n",
    "\n",
    "learning_rates = [1e-3]\n",
    "hidden_sizes = [1000]\n",
    "num_layerss = [1]\n",
    "batch_sizes = [128]\n",
    "kernel_sizes = [1]\n",
    "kernel_sizes2 = [1]\n",
    "F1_sizes = [32]\n",
    "F2_sizes = [64]\n",
    "dropout_probs = [0]\n",
    "\n",
    "skill_embedding_size=100\n",
    "certs_embedding_size=50\n",
    "license_embedding_size=10\n",
    "language_embedding_size=15\n",
    "address_embedding_size=25\n",
    "function_embedding_size=250\n",
    "isco4_embedding_size=150\n",
    "education_embedding_size=10\n",
    "isco_level_embedding_size=10\n",
    "company_embedding_size=300\n",
    "w2v_embedding_size = 300\n",
    "\n",
    "try:            \n",
    "    for learning_rate in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for num_layers in num_layerss:\n",
    "                for hidden_size in hidden_sizes:\n",
    "                    for kernel_size in kernel_sizes:\n",
    "                        for kernel_size2 in kernel_sizes2:\n",
    "                            for F1 in F1_sizes:\n",
    "                                for F2 in F2_sizes:\n",
    "                                    for dropout in dropout_probs:\n",
    "\n",
    "\n",
    "                                        lstm = CNN_LSTM(num_classes=num_classes,\n",
    "                                                        input_size=num_features,\n",
    "                                                        num_layers=num_layers,\n",
    "                                                        hidden_size=hidden_size,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        kernel_size2=kernel_size2,\n",
    "                                                        F1=F1,\n",
    "                                                        F2=F2,\n",
    "                                                        dropout=dropout,\n",
    "                                                        skills=skills, \n",
    "                                                        certs=certs,\n",
    "                                                        licenses=licenses,\n",
    "                                                        languages=languages,\n",
    "                                                        addresses=addresses,\n",
    "                                                        w2v=w2v,\n",
    "                                                        skill_embedding_size=skill_embedding_size,\n",
    "                                                        certs_embedding_size=certs_embedding_size,\n",
    "                                                        license_embedding_size=license_embedding_size,\n",
    "                                                        language_embedding_size=language_embedding_size,\n",
    "                                                        address_embedding_size=address_embedding_size,\n",
    "                                                        function_embedding_size=function_embedding_size,\n",
    "                                                        isco4_embedding_size=isco4_embedding_size,\n",
    "                                                        education_embedding_size=education_embedding_size,\n",
    "                                                        isco_level_embedding_size=isco_level_embedding_size,\n",
    "                                                        company_embedding_size=company_embedding_size,\n",
    "                                                        candidate_lengths=candidate_lens,\n",
    "                                                        max_len=max_len)\n",
    "\n",
    "                                        lstm = lstm.to(device)\n",
    "\n",
    "                                        optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "                                        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "                                        total = len(learning_rates) * len(num_layerss) * len(hidden_sizes) * len(batch_sizes) *\\\n",
    "                                                len(kernel_sizes) * len(kernel_sizes2) * len(F1_sizes) * len(F2_sizes) * len(dropout_probs)\n",
    "\n",
    "                                        print(f\"- Initial learning rate: {learning_rate}\\n- Model: \\n\\n\", lstm, \"\\n\")\n",
    "\n",
    "                                        trainloader, valloader, testloader = create_loaders(to_fill, idxs, y, split_size=0.8, \n",
    "                                                                                            weight_type=3, batch_size=batch_size)\n",
    "\n",
    "                                        # Store results of current configuration\n",
    "                                        outcome = train_loop(lstm, trainloader, valloader, testloader, optimizer, scheduler, criterion, num_epochs)\n",
    "                                        outcome[\"lr\"] = [learning_rate] * num_epochs\n",
    "                                        outcome[\"Batch size\"] = [batch_size] * num_epochs\n",
    "                                        outcome[\"Number of layers\"] = [num_layers] * num_epochs\n",
    "                                        outcome[\"Nodes per layer\"] = [hidden_size] * num_epochs\n",
    "                                        outcome[\"Kernel size\"] = [kernel_size] * num_epochs\n",
    "                                        outcome[\"Kernel size 2\"] = [kernel_size2] * num_epochs\n",
    "                                        outcome[\"F1 size\"] = [F1] * num_epochs\n",
    "                                        outcome[\"F2 size\"] = [F2] * num_epochs\n",
    "                                        outcome[\"Dropout\"] = [dropout] * num_epochs\n",
    "\n",
    "                                        full_results.append(outcome)\n",
    "\n",
    "                                        current += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f8079ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = defaultdict(list)\n",
    "\n",
    "for res in full_results:\n",
    "    for k, v in res.items():\n",
    "        merge_results[k].extend(v)\n",
    "        \n",
    "total = pd.DataFrame(merge_results).set_index([\"lr\", \"Batch size\", \"Number of layers\", \"Nodes per layer\", \n",
    "                                               \"Kernel size\", \"Kernel size 2\", \"F1 size\", \"F2 size\", \"Dropout\", \"Epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45088da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Acc@1</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>Acc@1 (test)</th>\n",
       "      <th>Acc@5 (test)</th>\n",
       "      <th>Acc@10 (test)</th>\n",
       "      <th>test_loss (test)</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Number of layers</th>\n",
       "      <th>Nodes per layer</th>\n",
       "      <th>Kernel size</th>\n",
       "      <th>Kernel size 2</th>\n",
       "      <th>F1 size</th>\n",
       "      <th>F2 size</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">0.001</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">128</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">1000</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">32</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">64</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.188341</td>\n",
       "      <td>0.455553</td>\n",
       "      <td>0.592808</td>\n",
       "      <td>3.657242</td>\n",
       "      <td>0.228544</td>\n",
       "      <td>0.513718</td>\n",
       "      <td>0.638146</td>\n",
       "      <td>3.480617</td>\n",
       "      <td>4.168038</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206190</td>\n",
       "      <td>0.473666</td>\n",
       "      <td>0.610217</td>\n",
       "      <td>3.607790</td>\n",
       "      <td>0.228016</td>\n",
       "      <td>0.516884</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>3.462938</td>\n",
       "      <td>2.228163</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.211730</td>\n",
       "      <td>0.481227</td>\n",
       "      <td>0.622175</td>\n",
       "      <td>3.691204</td>\n",
       "      <td>0.224059</td>\n",
       "      <td>0.529986</td>\n",
       "      <td>0.660834</td>\n",
       "      <td>3.531175</td>\n",
       "      <td>1.403730</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.213576</td>\n",
       "      <td>0.496615</td>\n",
       "      <td>0.632902</td>\n",
       "      <td>3.775222</td>\n",
       "      <td>0.237689</td>\n",
       "      <td>0.545023</td>\n",
       "      <td>0.670155</td>\n",
       "      <td>3.622058</td>\n",
       "      <td>1.023749</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.207157</td>\n",
       "      <td>0.488877</td>\n",
       "      <td>0.621736</td>\n",
       "      <td>3.916626</td>\n",
       "      <td>0.234787</td>\n",
       "      <td>0.533327</td>\n",
       "      <td>0.661713</td>\n",
       "      <td>3.762308</td>\n",
       "      <td>0.811996</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.215950</td>\n",
       "      <td>0.504001</td>\n",
       "      <td>0.639233</td>\n",
       "      <td>3.877276</td>\n",
       "      <td>0.245691</td>\n",
       "      <td>0.555487</td>\n",
       "      <td>0.679388</td>\n",
       "      <td>3.720387</td>\n",
       "      <td>0.620941</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.217005</td>\n",
       "      <td>0.507254</td>\n",
       "      <td>0.644157</td>\n",
       "      <td>3.904756</td>\n",
       "      <td>0.249912</td>\n",
       "      <td>0.560763</td>\n",
       "      <td>0.681762</td>\n",
       "      <td>3.745627</td>\n",
       "      <td>0.501437</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.222017</td>\n",
       "      <td>0.513321</td>\n",
       "      <td>0.648729</td>\n",
       "      <td>3.929307</td>\n",
       "      <td>0.254661</td>\n",
       "      <td>0.564808</td>\n",
       "      <td>0.682026</td>\n",
       "      <td>3.777029</td>\n",
       "      <td>0.438383</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.225182</td>\n",
       "      <td>0.514376</td>\n",
       "      <td>0.647323</td>\n",
       "      <td>3.963844</td>\n",
       "      <td>0.256947</td>\n",
       "      <td>0.565688</td>\n",
       "      <td>0.683257</td>\n",
       "      <td>3.815562</td>\n",
       "      <td>0.383378</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.516926</td>\n",
       "      <td>0.649785</td>\n",
       "      <td>3.994619</td>\n",
       "      <td>0.264597</td>\n",
       "      <td>0.565072</td>\n",
       "      <td>0.683433</td>\n",
       "      <td>3.849970</td>\n",
       "      <td>0.346016</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.225182</td>\n",
       "      <td>0.517366</td>\n",
       "      <td>0.650048</td>\n",
       "      <td>3.997686</td>\n",
       "      <td>0.263454</td>\n",
       "      <td>0.564720</td>\n",
       "      <td>0.683961</td>\n",
       "      <td>3.853151</td>\n",
       "      <td>0.315683</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.225798</td>\n",
       "      <td>0.516311</td>\n",
       "      <td>0.650928</td>\n",
       "      <td>4.001789</td>\n",
       "      <td>0.263278</td>\n",
       "      <td>0.565512</td>\n",
       "      <td>0.684664</td>\n",
       "      <td>3.856535</td>\n",
       "      <td>0.311411</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.226677</td>\n",
       "      <td>0.517454</td>\n",
       "      <td>0.650048</td>\n",
       "      <td>4.008154</td>\n",
       "      <td>0.264421</td>\n",
       "      <td>0.564720</td>\n",
       "      <td>0.684312</td>\n",
       "      <td>3.863145</td>\n",
       "      <td>0.304484</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.227029</td>\n",
       "      <td>0.517981</td>\n",
       "      <td>0.649257</td>\n",
       "      <td>4.011797</td>\n",
       "      <td>0.264246</td>\n",
       "      <td>0.564544</td>\n",
       "      <td>0.684312</td>\n",
       "      <td>3.867450</td>\n",
       "      <td>0.297223</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.226677</td>\n",
       "      <td>0.517278</td>\n",
       "      <td>0.649609</td>\n",
       "      <td>4.017975</td>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.564544</td>\n",
       "      <td>0.685807</td>\n",
       "      <td>3.873375</td>\n",
       "      <td>0.291867</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.226941</td>\n",
       "      <td>0.517542</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>4.017967</td>\n",
       "      <td>0.263982</td>\n",
       "      <td>0.564632</td>\n",
       "      <td>0.685719</td>\n",
       "      <td>3.874083</td>\n",
       "      <td>0.290058</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.227381</td>\n",
       "      <td>0.517893</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>4.019338</td>\n",
       "      <td>0.263718</td>\n",
       "      <td>0.565248</td>\n",
       "      <td>0.685456</td>\n",
       "      <td>3.875101</td>\n",
       "      <td>0.283519</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.227293</td>\n",
       "      <td>0.517981</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>4.019776</td>\n",
       "      <td>0.263718</td>\n",
       "      <td>0.565248</td>\n",
       "      <td>0.686247</td>\n",
       "      <td>3.874875</td>\n",
       "      <td>0.285828</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.227556</td>\n",
       "      <td>0.517717</td>\n",
       "      <td>0.649257</td>\n",
       "      <td>4.020553</td>\n",
       "      <td>0.264333</td>\n",
       "      <td>0.565336</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>3.876072</td>\n",
       "      <td>0.283769</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.227556</td>\n",
       "      <td>0.517278</td>\n",
       "      <td>0.649433</td>\n",
       "      <td>4.020477</td>\n",
       "      <td>0.264421</td>\n",
       "      <td>0.564720</td>\n",
       "      <td>0.685807</td>\n",
       "      <td>3.876754</td>\n",
       "      <td>0.287252</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              Acc@1  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch             \n",
       "0.001 128        1                1000            1           1             32      64      0       1      0.188341   \n",
       "                                                                                                    2      0.206190   \n",
       "                                                                                                    3      0.211730   \n",
       "                                                                                                    4      0.213576   \n",
       "                                                                                                    5      0.207157   \n",
       "                                                                                                    6      0.215950   \n",
       "                                                                                                    7      0.217005   \n",
       "                                                                                                    8      0.222017   \n",
       "                                                                                                    9      0.225182   \n",
       "                                                                                                    10     0.226062   \n",
       "                                                                                                    11     0.225182   \n",
       "                                                                                                    12     0.225798   \n",
       "                                                                                                    13     0.226677   \n",
       "                                                                                                    14     0.227029   \n",
       "                                                                                                    15     0.226677   \n",
       "                                                                                                    16     0.226941   \n",
       "                                                                                                    17     0.227381   \n",
       "                                                                                                    18     0.227293   \n",
       "                                                                                                    19     0.227556   \n",
       "                                                                                                    20     0.227556   \n",
       "\n",
       "                                                                                                              Acc@5  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch             \n",
       "0.001 128        1                1000            1           1             32      64      0       1      0.455553   \n",
       "                                                                                                    2      0.473666   \n",
       "                                                                                                    3      0.481227   \n",
       "                                                                                                    4      0.496615   \n",
       "                                                                                                    5      0.488877   \n",
       "                                                                                                    6      0.504001   \n",
       "                                                                                                    7      0.507254   \n",
       "                                                                                                    8      0.513321   \n",
       "                                                                                                    9      0.514376   \n",
       "                                                                                                    10     0.516926   \n",
       "                                                                                                    11     0.517366   \n",
       "                                                                                                    12     0.516311   \n",
       "                                                                                                    13     0.517454   \n",
       "                                                                                                    14     0.517981   \n",
       "                                                                                                    15     0.517278   \n",
       "                                                                                                    16     0.517542   \n",
       "                                                                                                    17     0.517893   \n",
       "                                                                                                    18     0.517981   \n",
       "                                                                                                    19     0.517717   \n",
       "                                                                                                    20     0.517278   \n",
       "\n",
       "                                                                                                             Acc@10  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch             \n",
       "0.001 128        1                1000            1           1             32      64      0       1      0.592808   \n",
       "                                                                                                    2      0.610217   \n",
       "                                                                                                    3      0.622175   \n",
       "                                                                                                    4      0.632902   \n",
       "                                                                                                    5      0.621736   \n",
       "                                                                                                    6      0.639233   \n",
       "                                                                                                    7      0.644157   \n",
       "                                                                                                    8      0.648729   \n",
       "                                                                                                    9      0.647323   \n",
       "                                                                                                    10     0.649785   \n",
       "                                                                                                    11     0.650048   \n",
       "                                                                                                    12     0.650928   \n",
       "                                                                                                    13     0.650048   \n",
       "                                                                                                    14     0.649257   \n",
       "                                                                                                    15     0.649609   \n",
       "                                                                                                    16     0.649521   \n",
       "                                                                                                    17     0.649521   \n",
       "                                                                                                    18     0.649521   \n",
       "                                                                                                    19     0.649257   \n",
       "                                                                                                    20     0.649433   \n",
       "\n",
       "                                                                                                           test_loss  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch              \n",
       "0.001 128        1                1000            1           1             32      64      0       1       3.657242   \n",
       "                                                                                                    2       3.607790   \n",
       "                                                                                                    3       3.691204   \n",
       "                                                                                                    4       3.775222   \n",
       "                                                                                                    5       3.916626   \n",
       "                                                                                                    6       3.877276   \n",
       "                                                                                                    7       3.904756   \n",
       "                                                                                                    8       3.929307   \n",
       "                                                                                                    9       3.963844   \n",
       "                                                                                                    10      3.994619   \n",
       "                                                                                                    11      3.997686   \n",
       "                                                                                                    12      4.001789   \n",
       "                                                                                                    13      4.008154   \n",
       "                                                                                                    14      4.011797   \n",
       "                                                                                                    15      4.017975   \n",
       "                                                                                                    16      4.017967   \n",
       "                                                                                                    17      4.019338   \n",
       "                                                                                                    18      4.019776   \n",
       "                                                                                                    19      4.020553   \n",
       "                                                                                                    20      4.020477   \n",
       "\n",
       "                                                                                                           Acc@1 (test)  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch                 \n",
       "0.001 128        1                1000            1           1             32      64      0       1          0.228544   \n",
       "                                                                                                    2          0.228016   \n",
       "                                                                                                    3          0.224059   \n",
       "                                                                                                    4          0.237689   \n",
       "                                                                                                    5          0.234787   \n",
       "                                                                                                    6          0.245691   \n",
       "                                                                                                    7          0.249912   \n",
       "                                                                                                    8          0.254661   \n",
       "                                                                                                    9          0.256947   \n",
       "                                                                                                    10         0.264597   \n",
       "                                                                                                    11         0.263454   \n",
       "                                                                                                    12         0.263278   \n",
       "                                                                                                    13         0.264421   \n",
       "                                                                                                    14         0.264246   \n",
       "                                                                                                    15         0.263190   \n",
       "                                                                                                    16         0.263982   \n",
       "                                                                                                    17         0.263718   \n",
       "                                                                                                    18         0.263718   \n",
       "                                                                                                    19         0.264333   \n",
       "                                                                                                    20         0.264421   \n",
       "\n",
       "                                                                                                           Acc@5 (test)  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch                 \n",
       "0.001 128        1                1000            1           1             32      64      0       1          0.513718   \n",
       "                                                                                                    2          0.516884   \n",
       "                                                                                                    3          0.529986   \n",
       "                                                                                                    4          0.545023   \n",
       "                                                                                                    5          0.533327   \n",
       "                                                                                                    6          0.555487   \n",
       "                                                                                                    7          0.560763   \n",
       "                                                                                                    8          0.564808   \n",
       "                                                                                                    9          0.565688   \n",
       "                                                                                                    10         0.565072   \n",
       "                                                                                                    11         0.564720   \n",
       "                                                                                                    12         0.565512   \n",
       "                                                                                                    13         0.564720   \n",
       "                                                                                                    14         0.564544   \n",
       "                                                                                                    15         0.564544   \n",
       "                                                                                                    16         0.564632   \n",
       "                                                                                                    17         0.565248   \n",
       "                                                                                                    18         0.565248   \n",
       "                                                                                                    19         0.565336   \n",
       "                                                                                                    20         0.564720   \n",
       "\n",
       "                                                                                                           Acc@10 (test)  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch                  \n",
       "0.001 128        1                1000            1           1             32      64      0       1           0.638146   \n",
       "                                                                                                    2           0.652832   \n",
       "                                                                                                    3           0.660834   \n",
       "                                                                                                    4           0.670155   \n",
       "                                                                                                    5           0.661713   \n",
       "                                                                                                    6           0.679388   \n",
       "                                                                                                    7           0.681762   \n",
       "                                                                                                    8           0.682026   \n",
       "                                                                                                    9           0.683257   \n",
       "                                                                                                    10          0.683433   \n",
       "                                                                                                    11          0.683961   \n",
       "                                                                                                    12          0.684664   \n",
       "                                                                                                    13          0.684312   \n",
       "                                                                                                    14          0.684312   \n",
       "                                                                                                    15          0.685807   \n",
       "                                                                                                    16          0.685719   \n",
       "                                                                                                    17          0.685456   \n",
       "                                                                                                    18          0.686247   \n",
       "                                                                                                    19          0.685983   \n",
       "                                                                                                    20          0.685807   \n",
       "\n",
       "                                                                                                           test_loss (test)  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch                     \n",
       "0.001 128        1                1000            1           1             32      64      0       1              3.480617   \n",
       "                                                                                                    2              3.462938   \n",
       "                                                                                                    3              3.531175   \n",
       "                                                                                                    4              3.622058   \n",
       "                                                                                                    5              3.762308   \n",
       "                                                                                                    6              3.720387   \n",
       "                                                                                                    7              3.745627   \n",
       "                                                                                                    8              3.777029   \n",
       "                                                                                                    9              3.815562   \n",
       "                                                                                                    10             3.849970   \n",
       "                                                                                                    11             3.853151   \n",
       "                                                                                                    12             3.856535   \n",
       "                                                                                                    13             3.863145   \n",
       "                                                                                                    14             3.867450   \n",
       "                                                                                                    15             3.873375   \n",
       "                                                                                                    16             3.874083   \n",
       "                                                                                                    17             3.875101   \n",
       "                                                                                                    18             3.874875   \n",
       "                                                                                                    19             3.876072   \n",
       "                                                                                                    20             3.876754   \n",
       "\n",
       "                                                                                                           training_loss  \\\n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch                  \n",
       "0.001 128        1                1000            1           1             32      64      0       1           4.168038   \n",
       "                                                                                                    2           2.228163   \n",
       "                                                                                                    3           1.403730   \n",
       "                                                                                                    4           1.023749   \n",
       "                                                                                                    5           0.811996   \n",
       "                                                                                                    6           0.620941   \n",
       "                                                                                                    7           0.501437   \n",
       "                                                                                                    8           0.438383   \n",
       "                                                                                                    9           0.383378   \n",
       "                                                                                                    10          0.346016   \n",
       "                                                                                                    11          0.315683   \n",
       "                                                                                                    12          0.311411   \n",
       "                                                                                                    13          0.304484   \n",
       "                                                                                                    14          0.297223   \n",
       "                                                                                                    15          0.291867   \n",
       "                                                                                                    16          0.290058   \n",
       "                                                                                                    17          0.283519   \n",
       "                                                                                                    18          0.285828   \n",
       "                                                                                                    19          0.283769   \n",
       "                                                                                                    20          0.287252   \n",
       "\n",
       "                                                                                                           duration  \n",
       "lr    Batch size Number of layers Nodes per layer Kernel size Kernel size 2 F1 size F2 size Dropout Epoch            \n",
       "0.001 128        1                1000            1           1             32      64      0       1           348  \n",
       "                                                                                                    2           349  \n",
       "                                                                                                    3           349  \n",
       "                                                                                                    4           348  \n",
       "                                                                                                    5           348  \n",
       "                                                                                                    6           349  \n",
       "                                                                                                    7           348  \n",
       "                                                                                                    8           348  \n",
       "                                                                                                    9           348  \n",
       "                                                                                                    10          348  \n",
       "                                                                                                    11          349  \n",
       "                                                                                                    12          350  \n",
       "                                                                                                    13          350  \n",
       "                                                                                                    14          348  \n",
       "                                                                                                    15          348  \n",
       "                                                                                                    16          349  \n",
       "                                                                                                    17          349  \n",
       "                                                                                                    18          349  \n",
       "                                                                                                    19          349  \n",
       "                                                                                                    20          349  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca00aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv(\"../results/CNN-LSTM-results_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd58d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 0.25\n",
      "Previous-job baseline accuracy: 0.0\n",
      "Fraction of previous job predictions: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for candidate, career, job in valloader:\n",
    "        career, job = career.to(device), job.to(device)\n",
    "        pred = lstm(candidate, career)\n",
    "        \n",
    "        print(\"Batch accuracy:\", (pred.argmax(1) == job).type(torch.float).mean().item())\n",
    "        \n",
    "        # Check how often the model predicted the previous job + compare to baseline performance\n",
    "        previous_job = torch.Tensor(career_paths.loc[candidate.cpu()].apply(lambda x: x[-2][-1]).values).to(device)\n",
    "        print(\"Previous-job baseline accuracy:\", (job == previous_job).cpu().numpy().mean())\n",
    "        print(\"Fraction of previous job predictions:\", (pred.argmax(1) == previous_job).cpu().numpy().mean())\n",
    "                \n",
    "        a = pd.Series(Counter(job.tolist()))\n",
    "        a.sort_index().plot(kind=\"area\", label=\"Ground truth\")\n",
    "        \n",
    "        b = pd.Series(Counter(pred.argmax(1).tolist()))\n",
    "        b.sort_index().plot(kind=\"area\", label=\"predicted\")\n",
    "        \n",
    "        plt.xlabel(\"isco_code4\")\n",
    "        plt.ylabel(\"number of occurences\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7fb3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.685807\n",
    "N = len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1c3e15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11372"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3d24295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006094089378759572"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(1.96 * ((1 - acc) * acc) / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb92de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
